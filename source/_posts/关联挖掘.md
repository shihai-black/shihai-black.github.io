---
title: 关联挖掘
categories:
  - null
tags:
  - 机器学习
mathjax: true
date: 2021-09-18 16:46:37
description:
---

> 对商品订单进行类目级别的关联挖掘，因为订单太少，无法用到商品这个级别，并且主要是因为做关联挖掘的目的是为了投放广告

## 指标定义

**项于项集**

项，指我们分析数据中的**一个对象**，如啤酒；项集，就是**若干项**的项构成的集合，如集合{啤酒，面包}是一个2项集。

**支持度（support）**

某项集在数据集中出现的概率，支持度体现的是某个项集的频繁程度，只有**项集达到一定程度**，我们才会研究该项集的必要。

其计算公式为：

{% raw %}
$$
support(A\cup B)=\frac{{A\cup B}}{All}
$$
{% endraw %}

举个例子：AB在订单中同时出现了100次，总共有1000个订单，那$A\cup B$项集的支持度就是0.1

**置信度（confidence）**

当用户购买了商品A后有多大概率会购买商品B，表示关联性的强弱。

其计算公式为:
{% raw %}
$$
confidence(A\rightarrow B)=\frac{support(A\cup B)}{support(A)}
$$
{% endraw %}
举个例子，AB同时出现的订单有100次，而A购买的订单次数有200次，那置信度就是0.5

**提升度（lift）**

商品A的出现，对商品B的出现概率提升的程度，一般是大于1说明A的出现有组与商品B的出现

其计算公式为：

{% raw %}
$$
lift(A\rightarrow B) = \frac{confidence(A\rightarrow B)}{support(B)}
$$
{% endraw %}

举个例子，已知总订单100，AB同时出现时20：

- 如果B的订单是20，A的订单时40，那么提升度为5，说明B商品强依赖于A，A的出现有利于B的出现
- 同样，如果B的订单是50，A的订单是50，那么提升度为0.8，说明B商品和A商品没有强依赖关系。

## 经典算法

### Apriori

具体步骤

1. 计算项集K=1的所有支持度
2. 选出大于最小支持度的数据
3. 如果项集为空，则对应 K-1 项集的结果为最终结果。否则重复1，2

该算法时间负责度比较高，相当于，针对每个K都要计算一次数据集。因此用学者利用树来优化算法

### FP-Growth 

具体步骤

1. 扫描数据，得到所有频繁1项集的的计数。然后删除支持度低于阈值的项，将1项集放入项头表，并按照支持度降序排列
2. 扫描数据，将读到的原始数据剔除非频繁1项集，并按照支持度降序排列。
3. 读入排序后的数据集，插入FP树，插入时按照排序后的顺序，插入FP树中，排序靠前的节点是祖先节点，而靠后的是子孙节点。如果有共用的祖先，则对应的公用祖先节点计数加1。插入后，如果有新节点出现，则项头表对应的节点会通过节点链表链接上新节点。直到所有的数据都插入到FP树后，FP树的建立完成。
4. 从项头表的底部项依次向上找到项头表项对应的条件模式基。从条件模式基递归挖掘得到项头表项项的频繁项集。
5. 如果不限制频繁项集的项数，则返回步骤4所有的频繁项集，否则只返回满足项数要求的频繁项集。

该算法只需要扫面两次数据集就可以得出所有结果，因此相较于Apriori速度会有所提神，不过也得看具体可以提出多大的K

## 代码

对于apriori主要有两个包可以用efficient_apriori,mlxtend.frequent_patterns。前者是比较方便，但是问题在于没有具体的数字，后面这个包就比较舒服了，可以直接输出对应的指标。不过主要是我现在用的数据集小，如果数据集很大，那可能FP-Growth更合适一些不过要看具体场景,他的包fptools。

```python
from efficient_apriori import apriori
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori as apr
from mlxtend.frequent_patterns import association_rules
import pandas as pd
import fptools as fp


data=[('牛奶','面包','尿布'),('可乐','面包', '尿布', '啤酒'),
        ('牛奶','尿布', '啤酒', '鸡蛋'),
        ('面包', '牛奶', '尿布', '啤酒'),
        ('面包', '牛奶', '尿布', '可乐')]

# efficient_apriori包的使用
itemsets, rules = apriori(data, min_support=0.5,  min_confidence=1)

# mlxtend.frequent_patterns包的使用
Encoder = TransactionEncoder()
encoded_data = Encoder.fit_transform(data)
df = pd.DataFrame(encoded_data, columns=Encoder.columns_)
frequent_items = apr(df, min_support=0.5, use_colnames=True, max_len=4).sort_values(by='support', ascending=False)
df_re = association_rules(frequent_items, metric='lift', min_threshold=1)

#fp
data = [('牛奶','面包','尿布'),
('可乐','面包', '尿布', '啤酒'),
('牛奶','尿布', '啤酒', '鸡蛋'),
('面包', '牛奶', '尿布', '啤酒'),
('面包', '牛奶', '尿布', '可乐')]
fis = [iset for iset in fp.frequent_itemsets(data, 2)]
mfis = [iset for iset in fp.maximal_frequent_itemsets(data, 2)]
```

## 参考

[知乎1](https://zhuanlan.zhihu.com/p/198842818)

[知乎2](https://zhuanlan.zhihu.com/p/66944900)

[博客](https://www.cnblogs.com/zhengxingpeng/p/6679280.html)

