---
title: 异常检测整理
date: 2021-04-08 16:25:05
categories:
- 机器学习
tags:
- 异常检测
description:
- 此文档写于2020年，建成于博客创立之前。
---

## 前言

**定义**：识别不正常情况与挖掘非逻辑数据的技术，也叫outliers。

**前提**：

1. 异常数据只占少数
2. 异常数据特征值和正常数据差别很大

**应用领域**：

1. CV领域:抖音发现违规视频
2. 数据挖掘：信用卡盗刷，支付宝，异常金额支出。

**模型**

1. 无监督学习、AutoEncoder、GAN、矩阵因子分解
2. 半监督学习，强化学习
3. hybrid（混种）、特征提取+传统算法
4. 单分类神经网路(MLM)

## 统计学方法

### 3sigma/箱形图

**原理**：远离3sigma（拉依达准则）数据概率低于0.01，认为这些数据为异常值

**缺点**：

1. 要保证异常值较少
2. 只能检测单维数据
3. 要假定数据服从正态分布或近似

### 高斯概率密度异常检测算法（1999）

**原理**：首先，该算法假设数据集服从高斯分布的，然后再分别计算训练集在空间中的重心, 和方差, 然后根据高斯概率密度估算每个点被分配到重心的概率，进而完成异常检测任务。(感觉和3sigma想法很像)

**缺点**：

1. 不适用于高维特征数据集
2. 要求数据大致服从高斯分布的数据集

## 无监督学习

### Isolation Forest(孤立森林)

**定义**：孤立森林是用于异常检测的机器学习算法。这是一种无监督学习算法，通过隔离数据中的离群值识别异常

**原理**：孤立森林通过**随机选择特征**，然后**随机选择**特征的**分割值**，递归地生成数据集的分区。和数据集中「正常」的点相比，要隔离的异常值所需的随机分区更少，因此**异常值是树中路径更短的点**，路径长度是从根节点经过的边数。

**重要参数**

- n_estimators：树的数量
- max_sample:样本抽样（小样本全抽）
- contamination：异常占比，这个值很关键
- max_features：随机选取特征维度

**具体步骤**：

1. 从数据集中按max_sample进行抽样
2. 随机指定部分维度（论文是只用一个维度），在当前节点数据中随机产生一个切割点p——切割点产生于当前节点数据中指定维度的最大值和最小值之间。
3. 以此切割点生成了一个超平面，然后将当前节点数据空间划分为2个子空间：把指定维度里小于p的数据放在当前节点的左边，把大于等于p的数据放在当前节点的右边。
4. 在子节点中递归步骤2和3，不断构造新的子节点，直到子节点中只有一个数据（无法再继续切割）或子节点已到达限定高度（算法设定的）。

**优点**：

1. 节省内存。由于其主要定位异常值，因此其不要求树全部描述出来，可以用最大深度来限制。并且其不像kmeans等需要计算有关距离、密度的指标，可大幅度提升速度。
2. 适用于小数据集：因此采样，如果数据集很大，
3. 集成算法，多个专家的树针对不同的异常

**缺点：**

1. 如果只能用1个维度，那对于图片这种高维特征，效果就不佳。

**参考**：

[Isolation Forest](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf )[2008]

[Isolation-based Anomaly Detection](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/tkdd11.pdf)[2012] 

### Local Outlier Factor（局部异常因子算法）

**定义**：一种典型的基于密度的高精度离群点检测算法

**原理**： LOF算法是通过比较每个点p和邻域点的密度来判断该点是否为异常：点p的密度越低，越有可能是异常点。而点的密度是通过点之间的距离来计算的，点之间距离越远，密度越低；距离越近，密度越高。也就是说，LOF算法中点的密度是通过点的k邻域计算得到的，而不是通过全局计算得到，这里的"k邻域”也就是该算法中“局部”的概念。

**重要参数**

- n_neighbors：上文的K，检测的领域点个数超过样本数则使用所有的样本进行检测
- contamination：异常占比，这个值很关键
- metric:距离度量单位
- p=2：距离度量单位（l1,l2分别为1,2）

**具体步骤**：主要就是计算LOF，如果想弄明白怎么算的直接看源论文

**优点**

1. 适用于对不同密度的数据的异常检测

**缺点**

1. 检测的数据必须有明显的密度差异，计算较为复杂

**参考**

[Local Outlier Factor](https://www.dbs.ifi.lmu.de/Publikationen/Papers/LOF.pdf )[2000]

### One Class SVM（新颖点检测算法）

**定义**：无监督算法，将数据分类为不同的类型

**原理**：与SVM类似，SVM是寻找一个超平面，使用这个超平面把正常数据和异常数据划分开。而One_class SVM是基于一类数据（正常数据）求超平面，对SVM算法中求解负样本最大间隔目标进行改造。

**优点**：

1. 适用于高纬数据集，毕竟是超平面

### DBSCAN（密度聚类）

**定义**：无监督聚类算法，按照密度将空间内的数据进行聚类，如果单独成簇就是异常值。

**原理**：给定一个距离半径和类内最少多少个点，然后把可以满足的点全部都连起来，判定为同类。

**优点**：

1. 不需要知道K，按距离自动划分为簇
2. 能发现任意非球形状的簇类
3. 对输入样本的顺序并不敏感

**缺点**：

1. 不适用于高纬数据
2. 时间复杂度较高

**参考**

[DBSCAN](http://www2.cs.uh.edu/~ceick/7363/Papers/dbscan.pdf)

## 降维

### PCA

**定义**：主成分分析（PCA），一种使用广泛的数据降维算法，主要思想是将n维特征映射到k维上，k维是全新的正交特征也称为主成分。

**原理**：PCA的工作就是从原始的空间中顺序地寻找一组相互正交的坐标轴，第一个新坐标轴选择是原始数据中**方差最大**的方向，第二个新坐标轴选取是与第一个坐标轴正交的平面中使得方差最大的，第三个轴是与第1,2个轴正交的平面中方差最大的。依次类推，可以得到n个这样的坐标轴。取前k个包含绝大部分方差的坐标轴。

**实现方式**

- 基于特征值分解协方差矩阵实现
- 基于SVD分解协方差矩阵（大样本高效，因此sklearn里面也是用svd分解）

**具体步骤**

1. 中心化（norm_x）
2. 获取协方差矩阵（np.dot(norm_xT,norm_x）)
3. 计算协方差矩阵的特征值和特征向量(feature_n,vectors_n)
4. 按特征值排序，选取最大的K组特征（特征值越大方差越大）(feature_k,vectors_k)
5. 反转，np.dot(norm_x,vectors_k)

**特点**

1. PCA后的特征不具有解释性
2. PCA不适合线性不可分数据

## 半监督学习（AE类）

### Auto-Encoder（鼻祖）

**背景**：AE的思想最早是1986年被提出来的

**定义**：一种无监督的学习算法，其可以解决PCA无法解决的问题，因为其可以对线性不可分的数据进行降维，利用反向传播算法，让目标等于输入值。

**原理**：构建一个函数使X和Y的Loss最小，保证降维后的图片保持最大的信息。

**重建误差**：就是损失函数，原始X和重建的Y之间的差异称为重建误差

**异常检测的使用**

1. 基于偏差的**半监督学习的**异常检测方法，使用**重建误差**作为异常分数，具有高重建误差作为异常。
2. 因此在训练阶段仅使用正常数据，训练之后AE可以很好的重建正常数据，而AE未遇到的异常数据则会重建失败（定位异常图片）
3. 为了增强鲁棒性一般会在正常数据里撒入一些噪声、或者随机将某些值变0（有点像dropout）。

**为什么叫Encoder，明明有Decoder**：

​	Vincent在2010的论文中做了研究，发现只要encoder单组W就可以，decoder中的W1可以用encoder中的W转置获得。其证明，W1没有任何作用，完全没有必要训练。

### Denoising Auto-Encoder（降噪自编码）

**背景**：Vincent在2008年的《Extracting and Composing Robust Features with Denoising Autoencoders》提出该模型

**做法**：对输入数据加入噪声，而输出数据是正常的数据，DAE会要求模型只去学习主要特征，输出的数据会有更好的鲁棒性

### Sparse Auto-Encoder（稀疏自编码）

**背景**：Andrew在2011年的《Sparse autoencoder》提出该模型

**做法**：在普通的AE的基础上增加了稀疏性约束，即要求神经元的平均输出较低，如果激活函数是sigmod，尽量让隐藏神经元输出为0，如果激活函数是tanh，尽量把输出变为-1.

> KL散度（相对熵）： 
>
> 1. 一种衡量两个概率分布的匹配程度的指标，两个分布差异越大，KL散度就越大。p目标分布，q是匹配分布，如果两个分布完全匹配，KL散度为0
> 2. KL散度是非对称的，即D（p||q）不一定等于D（q||p）
> 3. KL散度又叫相对熵，在信息论中，对于D（p||q）,描述的是q去拟合p产生的信息损耗

### Variational Auto-Encoder（VAE） 

**背景**：2014年，用于异常检测是2015年

**重建概率**：通过导出原始输入变量分布的参数的随机潜变量来计算重建概率

**损失函数**：重建概率+KL散度

**与AE的区别和联系**：

1. AE中的潜在变量由确定性映射定义，然而，由于VAE使用概率编码器来模拟潜在变量的分布，因此可以在采样过程中考虑潜在空间的可变性
2. 重建是随机变量，重建概率不仅考虑重建与原始输入之间的差异，而且还考虑分布函数的参数来重建的可变性。
3. 重建概率的计算不需要对异构数据进行处理，其重建误差的阈值比AE更客观且易于理解。

**异常检测的使用**

1. 一个半监督框架，仅使用正常实例的数据来训练VAE
2. 抽取样本，对于来自encoder的每个样本，概率解码器输出均值和方差参数，使用这些参数，计算从分布产生原始数据的概率。

### Conditional VAE

**背景**：一种框架，加入某种图片的先验信息，提升训练效果。

**优化点**：

- **多尺度预测目标**（内容很多，并行/串行，输入输出端……）：Loss=Loss1（1/4图）+Loss2（1/2图）+Loss3（原图）
- **对KL散度进行近似**：给KL散度增加罚函数，简单定为：batch_size/样本数量
- **增加label的one-hot**：对encode和decode的输入加入label的one-hot（conditional就是这一条，但对于一场检测没有意义）

**与VAE的联系与区别**：

1. 基本还是VAE的结构，对encoder和decoder的输入增加先验信息。
2. Loss函数进行了相应的调整。

### A Deep Hierarchical Variational Auto-Encoder

## 半监督学习（GAN类）

### AnoGAN

**背景**：GAN用于异常检测的开山之作，2017的论文

**基本思想**

1. 训练阶段：塞入正常图片，利用DCGAN训练一个模型，希望生成器能够生成足够好的正常图片，好到辨别器也无法判别他到底对不对。
2. 测试阶段：固定生成器和辨别器，他希望在Z的潜藏空间中找到一个和X最像的映射，然后利用梯度下降法，更新Z，生成一张由潜藏空间生成且和X最像的图片。
   - 定义一个损失函数，代表潜藏空间生成的图片和X的差异。
   - 随机抽一个Z，利用梯度下降法，不断更新使损失函数变最小，然后利用最好的Z生成图片进行异常分数计算（或者直接用）

**缺点：**

1. 对于X到Z的映射没有再训练阶段完成
2. z的更新非常耗时，并且每张图片都需要更新。
3. 该异常检测应该只适用于有严格边界的图像中。

**结果**：

原论文效果并不适用（周末试试用经典数据集进行训练）。可能因为生成模型训练的并不到位。但是loss也已经不变化了

### WGAN

**背景**：令GAN研究者眼前一亮的一种新模型，其从理论上很好的解决了GAN的几大问题.

**GAN的表面问题**

1. 训练不稳定，不容易收敛
   - 判别器训练太好，生成器梯度消失，loss不下降。
   - 判别器训练不好，生成器不稳定。
2. 生成图片的多样性不足。

**GAN的本质问题**

1. 等价优化的距离衡量（KL散度、JS散度）不合理（改进办法：用Wasserstein距离代替JS散度，合理解决前两个问题）
2. 生成分布于真实分布没有重叠部分（改进办法：增加噪声）

**WGAN和GAN的区别**

1. 判别器没有sigmoid，他不想DCGAN是2分类，他是回归问题
2. 生成器和判别器的loss不取log
3. 每次更新判别器的参数之后把它们的绝对值截断到不超过一个固定常数c
4. 不用Adam，用rmsprop做优化器

**WGAN_GP和WGAN的区别**

*WGAN的问题*

由于其限制每次判别器更新参数绝对值不超过阈值c,按原论文0.01，因此会出现大量参数集中在0.01和-0.01，这就导致所有参数基本都是0.01和-0.01。判别器性能就会变得很差。

*WGAN_GP的优势*

其设置一个额外的Loss来限制判别器的梯度，将参数更新正态分布到（-0.01，0.01）内。

*改变*

1. 优化器有可以用Adam了。。。
2. 由于其对每个样本独立的施加梯度惩罚，所以判别器的模型架构中不能使用Batch Normalization，改为使用Layer Normalization

### f-AnoGAN

**背景**：AnoGan的优化版本，2019年的论文。AnoGAN存在一个问题，每一张图片都需要不断迭代训练一个Z来代表其在隐空间内的点。接着利用训练好的DCGAN进行异常检测。由于其迭代优化势必导致时间加剧。而f-AnoGAN通过引入Encoder，解决了这个问题。

**基本思想**：

1. 输入正常图片，利用WGAN_GP进行训练生成器和判别器。

2. 输入正常图片，对Encoder+训练好的WGAN进行优化。提出了三种训练Encoder的方式（3种损失函数）

   - izi：image-z-image1，AutoEncoder样式的损失函数
   - ziz:z-image-z1，将Encoder放到Generate后面，比较z与z1的损失函数
   - izif：就是Anogan中的损失函数，从像素和特征两个维度比较图片结果。

   作者实验证明izif效果最好，并且整体训练也很快。

**结果**：

WGAN没有收敛，两边都不断降低，不确定是不是GP哪里有问题。

