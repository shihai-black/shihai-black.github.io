---
title: GNN系列之_GCN
categories:
  - 深度学习
tags:
  - GNNs
mathjax: true
date: 2021-08-19 11:05:12
description:
---

> GCN那段数学推断实在看不懂，不过作为一个worker我觉得一不一样数学原理都要搞的很透彻，毕竟我们只是模块的堆积和简单优化，推导还是交给顶尖大神吧

## 回顾CNN的

图片具有**局部平移不变性**：图片是一个规则的二维矩阵，无论卷积核平移到图片中的哪个位置都可以保证其运算结果的一致性，因此CNN可以利用卷积核进行特征提取。

**CNN的三大特点**

1. 参数共享：如果不共享，其实就是全连接，参数数据差距很大
2. 局部连接性：卷积计算每次只在与卷积核大小对应的区域进行
3. 层次化表达：不断叠加卷积层，每一个卷积层都是在前一层的基础上进行的，这样的意义在于，网络越往后，其提取到的特征越高级

## 什么是GCN

由于图片和文本等结构化数据具有上述提到的局部平移不变性，但很明显图的结构是不存在这种性质的，因为图结构的相邻节点数量是不确定的，因此用CNN的方式无法做卷积核。那graph中的CNN是什么呢？

目前的一大思路就是借助谱图理论（Spectral Graph Theory）来实现在拓扑图上的卷积操作，大致步骤为将空域中的拓扑图结构通过傅立叶变换映射到频域中并进行卷积，然后利用逆变换返回空域，从而完成了图卷积操作。

## **Graph Convolution**

17年Thomas N. Kipf的GCN模型的公式

$$
H^{(l+1)}=\sigma (\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}H^{l}W^{l})
$$


$\tilde{A}$：A+I 自连接的领结矩阵

$\tilde{D}$：度矩阵

W：参数【同层共享】，其作用就是将前一层的数据进行纬度变换

$\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}$:对领结矩阵做标准化，和$\tilde{D}^{-1}\tilde{A}$的区别就是前者用的是拉普拉斯变换，想较于加权求和取平均的方式而言，前者不但考虑当前节点的 i 的度，还考虑其他节点 j 的度。

## 结论

他不像CNN可能层数越多越能提取高纬信息，他一般就两层左右。其实我理解就是利用领结矩阵的信息，只是用的神经网络的模式，和SDNE的思想应该挺像的。所以他不训练的效果主要还是取决于领结矩阵信息的表达。并不是说他就直接比node2vec，SDNE等以前训练embedding的方式号。

**优点**

1. 即使不用训练，直接随机参数也可以获得不错的效果
2. GCN 可以在只标注少量样本的情况下学得出色的效果

**缺点**：

1. 要将整个图信息放入网络中，太占用内存和GPU
2. 无法用于新节点和新的图，属于直推式学习
3. 图数据很稀疏，训练和测试节点都是一样的

[1]: https://zhuanlan.zhihu.com/p/120311352	"知乎"
[2]: https://mp.weixin.qq.com/s/jBQOgP-I4FQT1EU8y72ICA
[3]: https://arxiv.org/abs/1606.09375	"GCN1"
[4]: https://arxiv.org/abs/1609.02907v4	"GCN2"

