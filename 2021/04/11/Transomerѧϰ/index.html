<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Transomer学习 | ShiHai'Blog</title><meta name="keywords" content="transfomer"><meta name="author" content="ShiHai-black"><meta name="copyright" content="ShiHai-black"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="此文档写于2019年，建成于博客创立之前。">
<meta property="og:type" content="article">
<meta property="og:title" content="Transomer学习">
<meta property="og:url" content="https://shihai-black.github.io/2021/04/11/Transomer%D1%A7%CF%B0/index.html">
<meta property="og:site_name" content="ShiHai&#39;Blog">
<meta property="og:description" content="此文档写于2019年，建成于博客创立之前。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg">
<meta property="article:published_time" content="2021-04-11T03:10:00.000Z">
<meta property="article:modified_time" content="2021-04-11T07:48:48.000Z">
<meta property="article:author" content="ShiHai-black">
<meta property="article:tag" content="transfomer">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://shihai-black.github.io/2021/04/11/Transomer%D1%A7%CF%B0/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-04-11 15:48:48'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    })(window)</script><meta name="generator" content="Hexo 5.4.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="/img/%E5%A4%B4%E5%83%8F.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">13</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">9</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">ShiHai'Blog</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 链接</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Transomer学习</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-04-11T03:10:00.000Z" title="发表于 2021-04-11 11:10:00">2021-04-11</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-04-11T07:48:48.000Z" title="更新于 2021-04-11 15:48:48">2021-04-11</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Transomer学习"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/2021/04/11/Transomer%D1%A7%CF%B0/#post-comment" itemprop="discussionUrl"><span class="valine-comment-count" data-xid="/2021/04/11/Transomer%D1%A7%CF%B0/" itemprop="commentCount"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="Transfomer拆分"><a href="#Transfomer拆分" class="headerlink" title="Transfomer拆分"></a>Transfomer拆分</h1><p>为了更好的学习当前NLP主流模型，如Bert，GPT2及Bert一系列的衍生物，Transfomer是这一系列的基础。因此本文的主要目的是记录个人基于一些博客和原论文对Transfomer模型进行拆分的结果。</p>
<p><strong>目的</strong>：减少计算量并提高并行效率，同时不减弱最终的实验结果。</p>
<p><strong>创新点</strong>：</p>
<ol>
<li>Self-attention</li>
<li>Multi-Head Attention</li>
</ol>
<h1 id="背景知识"><a href="#背景知识" class="headerlink" title="背景知识"></a>背景知识</h1><h2 id="seq2seq"><a href="#seq2seq" class="headerlink" title="seq2seq"></a>seq2seq</h2><p>定义：seq2seq模型是采用一系列项目(单词、字母、图像特征等)并输出另一个项目序列的模型。在机器翻译中，序列是一系列单词，经过seq2seq后，输出同样是一系列单词。</p>
<video src="F:\video\seq2seq_2.mp4"></video>

<p>接下来我们掀开这个model内部，该模型主要由一个Encoder和一个Decoder组成。</p>
<ul>
<li>Encoder：处理输入序列的每个项目，捕捉载体中的信息（context）。</li>
<li>Decoder：处理完整序列后，Encoder将信息（context）传递至Decoder，并且开始逐项生产输出序列。</li>
</ul>
<video src="F:\video\seq2seq_4.mp4"></video>

<p>而context是一个向量，其大小基于等同于编码器中RNN的隐藏神经元。</p>
<p>在单词输入之前，我们需要将单词转化为词向量，其可以通过word2vec等模型进行预训练训练词库，然后将单词按照词库的词向量简单提取即可，在上面的资历中，其处理过程如下：</p>
<p><img src="F:\video\embedding.png" alt="embedding"></p>
<p>这里简单将其设为维度4，通常设为200或300，在此，简单展示一下RNN的实现原理。</p>
<video src="F:\video\RNN_1.mp4"></video>

<p>利用先前的输入的隐藏状态，RNN将其输出至一个新的隐藏状态，接下来我们看看seq2seq中的隐藏状态是怎么进行的。</p>
<video src="F:\video\seq2seq_5.mp4"></video>

<p>Encoder中最后一个hidden-state实际就是前文提到的context，接下来我们进一步拆解，展示其具体细节。</p>
<video src="F:\video\seq2seq_6.mp4"></video>

<p>总结：由于在Encoder阶段，每个单词输入都会产生一个新的hidden_state，最后输出一个context给Decoder进行解码。因此，当文本内容过长时，容易丢失部分信息，为了解决这个问题，Attention应运而生。</p>
<h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p>Attention这个概念最早出现在《Neural machine traslation by jointly learning to align and translate》论文中，其后《Neural image caption generation with visual attention》对attention形式进行总结。</p>
<p>定义：为了解决文本过长信息丢失的问题，相较于seq2seq模型，attention最大的区别就是他不在要求把所以信息都编入最后的隐藏状态中，而是可以在编码过程中，对每一个隐藏状态进行保留，最后在解码的过程中，每一步都会选择性的从编码的隐藏状态中选一个和当前状态最接近的子集进行处理，这样在产生每一个输出时就能够充分利用输入序列携程的信息，下面很好的展示了attention在seq2seq模型中运用。</p>
<video src="F:\video\seq2seq_7.mp4"></video>

<p>接下来，我们放大一下decoder对于attention后隐藏状态的具体使用，其在每一步解码均要进行。</p>
<ol>
<li>查看在attention机制中每个具体的隐藏状态，选出其与句子中的那个单词最相关。</li>
<li>给每个隐藏状态打分。</li>
<li>将每个隐藏状态进行softmax以放大具有高分数的隐藏状态。</li>
<li>进行总和形成attention输入Decoder的向量。</li>
</ol>
<video src="F:\video\attention_process.mp4"></video>

<p>最后，将整个过程放一起，以便更好的理解attention机制(代码实现的时候进一步理解)。</p>
<ol>
<li>Decoder输入：初始化一个解码器隐藏状态+经过预训练后的词向量。</li>
<li>将前两者输入到RNN中，产生一个新的隐藏状态h4和输出，将输出丢弃(seq2seq是直接将context送入下一个RNN作为输入)。</li>
<li>attention：利用h4和encoder层简历的隐藏状态进行计算context（c4）。</li>
<li>将c4和h4进行concatenate。</li>
<li>将其结果通过前向神经网络，输出一个结果。</li>
<li>该结果表示当前时间输出的对应文字。</li>
<li>重复下一个步骤。</li>
</ol>
<video src="F:\video\attention_tensor_dance.mp4"></video>

<p>注意：该模型并不是将输入和输出的单词一一对应，他有可能一个单词对应两个单词甚至影响第三个单词，这是经过训练得到的。</p>
<h1 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h1><p>总算要写到Transformer部分了，有点小激动，让我们一起来看看这个影响到现在的模型到底长啥样，为了便于理解，我这边会结合代码+论文进行讲解。</p>
<p>首先，引入原论文的结构图</p>
<p><img src="F:\video\结构图.png" alt="结构图"></p>
<p>看不懂不要紧，这边引入代码的结构图</p>
<p><img src="F:\video\代码结构图.png" alt="代码结构图"></p>
<h2 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder-Decoder"></a>Encoder-Decoder</h2><p>从宏观角度来看，Transformer与Seq2Seq的结构相同，依然引入经典的Encoder-Decoder结构，只是其中的神经层已经不是以前的RNN和CNN，而是完全引入注意力机制来进行构建。</p>
<p><img src="F:\video\stack.png" alt="stack"></p>
<p>上图代码的复现结构</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderDecoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        整体来说Transformer还是Encoder和Decoder结构，其中包括两个Embedding，一块Encoder，一块Decoder，一个输出层</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, encoder, decoder, src_embed, tgt_embed, generator</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(EncoderDecoder, self).__init__()</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line">        self.src_embed = src_embed</span><br><span class="line">        self.tgt_embed = tgt_embed</span><br><span class="line">        self.generator = generator</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, src, tgt, src_mask, tgt_mask</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Take in and process masked src and target sequences.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span>(<span class="params">self, src, src_mask</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.encoder(self.src_embed(src), src_mask)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span>(<span class="params">self, memory, src_mask, tgt, tgt_mask</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)</span><br></pre></td></tr></table></figure>
<p>接下来，我将按照结构顺序一一介绍</p>
<h2 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h2><p>这一层没什么好说的倒是，就是一个Embedding层。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Embeddings</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">	<span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">		将单词转化为词向量</span></span><br><span class="line"><span class="string">	&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, vocab</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Embeddings, self).__init__()</span><br><span class="line">        self.lut = nn.Embedding(vocab, d_model)</span><br><span class="line">        self.d_model = d_model</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.lut(x) * math.sqrt(self.d_model)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>困惑</strong>：为什么要乘以sqrt(d_model),希望有大神给予指点！</p>
<h2 id="Positional-Emcoding"><a href="#Positional-Emcoding" class="headerlink" title="Positional Emcoding"></a>Positional Emcoding</h2><p>由于Transfomer完全引入注意力机制，其不像CNN和RNN会对输入单词顺序自动打上标签，其无法输出每个单词的顺序，在机器翻译中可是爆炸的啊，举个例子，你输入一句：我欠你的一千万不用还了，他返回一句：你欠我的一千万不用还了，那不是血崩。</p>
<p>为了解决这个问题，Transfomer在Encoder过程中为每个输入单词打上一个位置编码，然后在Decoder将位置变量信息添加，该方法不用模型训练，直接按规则进行添加。</p>
<p>我们看看原论文里的图</p>
<p><img src="F:\video\Emcodeing.png" alt="Emcodeing"></p>
<p>其计算的具体公式在原论文3.5，</p>
<p><img src="F:\video\Emcoding公式.png" alt="Emcoding公式"></p>
<p>对于上述公式，代码中进行了对数转化，具体如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    	位置变量公式详见https://arxiv.org/abs/1706.03762(3.5)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, dropout, max_len=<span class="number">5000</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        <span class="comment"># pe 初始化为0，shape为 [n, d_model] 的矩阵，用来存放最终的PositionalEncoding的</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        <span class="comment"># position 表示位置，shape为 [max_len, 1]，从0开始到 max_len</span></span><br><span class="line">        position = torch.arange(<span class="number">0.</span>, max_len).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 这个是变形得到的，shape 为 [1, d_model//2]</span></span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0.</span>, d_model, <span class="number">2</span>) * -(math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        <span class="comment"># 矩阵相乘 (max_len, 1) 与 (1, d_model // 2) 相乘，最后结果 shape   (max_len, d_model // 2)</span></span><br><span class="line">        <span class="comment"># 即所有行，一半的列。（行为句子的长度，列为向量的维度）</span></span><br><span class="line">        boy = position * div_term</span><br><span class="line">        <span class="comment"># 偶数列  奇数列 分别赋值</span></span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(boy)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(boy)</span><br><span class="line">        <span class="comment"># 为何还要在前面加一维？？？</span></span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># Parameter 会在反向传播时更新</span></span><br><span class="line">        <span class="comment"># Buffer不会，是固定的，本文就是不想让其被修改</span></span><br><span class="line">        self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># x能和后面的相加说明shape是一致的，也是 (1, sentence_len, d_model)</span></span><br><span class="line">        <span class="comment"># forward 其实就是，将原来的Embedding 再加上 Positional Embedding</span></span><br><span class="line">        x += Variable(self.pe[:, :x.size(<span class="number">1</span>)], requires_grad=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure>
<h2 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h2><p>Encoder代码结构</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;核心Encoder是由N层堆叠而成&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, layer, N</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, mask</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        将输入x和掩码mask逐层传递下去，最后再 LayerNorm 一下。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure>
<p>Encoder块主要就是利用Clone函数重复构建相同结构的Encoder_layer.</p>
<h3 id="Clone"><a href="#Clone" class="headerlink" title="Clone"></a>Clone</h3><p>整个Encoder部分由N个Encoder_layer堆积二乘，为了复刻每个层的结构，构建克隆函数。(Decoder类似)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clones</span>(<span class="params">module, N</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    	copyN层形成一个list</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N)])</span><br></pre></td></tr></table></figure>
<h3 id="Encoder-layer"><a href="#Encoder-layer" class="headerlink" title="Encoder_layer"></a>Encoder_layer</h3><p>接下来，我们在将独立的Encoder_layer进行拆分，看看里面到底是什么东西。</p>
<p><img src="F:\video\encoderlayer.png" alt="encoderlayer"></p>
<p>上图很清晰的展示Encoder_layer内部的结构，X1，X2是经过转化的词向量，经过Positional Emcoding进入Encider_layer,喝口水，这部分要讲的有点多……</p>
<p>首先，Encoder_layer分为上下两层，第一层包含self-attention+SublayerConnection,第二层为FNN+SublayerConnection。Attention的内容我后面再说，这里先讲下什么是SublayerConnection。attention的内容我后面再说，这里先讲下什么是SublayerConnection。</p>
<p><strong>SublayerConnection</strong></p>
<p>SublayerConnection内部设计基于两个核心：</p>
<ol>
<li>Residual_Connection（残差连接），这是上图的Add</li>
<li>Layer Normalize(层级归一化)，这是上图的Normalize</li>
</ol>
<p>如下图所示，残差连接，就是在原图正常的结构下，将X进行保留，最终得出的结果是X+F(x).</p>
<p><strong>优势</strong>：反向传播过程中，对X求导会多出一个常数1，梯度连乘，不会出现梯度消失(详细的内容等我以后探究一下)</p>
<p><img src="F:\video\Residual connection.png" alt="Residual connection"></p>
<p>具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SublayerConnection</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">		A residual connection followed by a layer norm.Note for code simplicity the norms is first as opposed to last.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, size, dropout</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SublayerConnection, self).__init__()</span><br><span class="line">        self.norm = LayerNorm(size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, sublayer</span>):</span></span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sublayer(self.norm(x)))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>Layer Normalize</strong></p>
<p>归一化最简单的理解就是将输入转化为均值为0，方差为1的数据，而由于NLP中Sequence长短不一，LN相较于BN在RNN这种类似的结构效果会好很多。其具体的概括就是：对每一个输入样本进行归一化操作，保证该样本在多维度符合均值0，方差为1.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LayerNorm</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;构建一个 layernorm层，具体细节看论文 https://arxiv.org/abs/1607.06450</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, features, eps=<span class="number">1e-6</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LayerNorm, self).__init__()</span><br><span class="line">        self.a_2 = nn.Parameter(torch.ones(features))</span><br><span class="line">        self.b_2 = nn.Parameter(torch.zeros(features))</span><br><span class="line">        self.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        mean = x.mean(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        std = x.std(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self.a_2 * (x - mean) / (std + self.eps) + self.b_2</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>FNN</strong></p>
<p>前馈神经网络，这就不细说了，具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionwiseFeedForward</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, d_ff, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        self.w_1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        self.w_2 = nn.Linear(d_ff, d_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.w_2(self.dropout(F.relu(self.w_1(x))))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>放张图，清晰一点</p>
<p><img src="F:\video\FNN.png" alt="FNN"></p>
<h2 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h2><p>与Encoder结构类似，其由N个Decoder_layer组成(Transformer中N=6),相较于Encoder,其接受的参数多了Encoder生成的memory以及目标句子中的掩码gt_mask.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">	<span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">		Gener is N layer decoder with masking</span></span><br><span class="line"><span class="string">	&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, layer, N</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, memory, src_mask, tgt_mask</span>):</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, memory, src_mask, tgt_mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure>
<h3 id="Decoder-layer"><a href="#Decoder-layer" class="headerlink" title="Decoder layer"></a>Decoder layer</h3><p>下图很好的表明了Decoder_layer的区别</p>
<p><img src="F:\video\Decoder.png" alt="Decoder"></p>
<p>简单的解释一下，每一个Decoder_layer有三层组成</p>
<p>第一层：Self-Attention+SublayerConnection</p>
<p>第二层：Encoder-Decoder Attention+SublayerConnection(Decoder独有)</p>
<p>第三层：FFN+SublayerConnection</p>
<p>具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, size, self_attn, src_attn, feed_forward, dropout</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer, self).__init__()</span><br><span class="line">        self.size = size</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.src_attn = src_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, memory, src_mask, tgt_mask</span>):</span></span><br><span class="line">        m = memory</span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, tgt_mask))</span><br><span class="line">        x = self.sublayer[<span class="number">1</span>](x, <span class="keyword">lambda</span> x: self.src_attn(x, m, m, src_mask))</span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">2</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure>
<h3 id="Mask"><a href="#Mask" class="headerlink" title="Mask"></a>Mask</h3><p><strong>作用：</strong>Mask简单来说就是掩码的意思，在我们这里的意思大概就是对某些值进行掩盖，不使其产生效果。</p>
<p>文中的Mask主要包括<strong>两种</strong>：</p>
<ul>
<li>src_mask(padding_mask):由于Sequence长短不一，利用Padding对其填充为0，然后在对其进行attention的过程中，这些位置没有意义的，src_mask的作用就是不将Attention机制放在这些位置上进行处理，这就是padding_mask,其基于作用于所有attention。</li>
<li>tgt_mask(sequence_mask):对于机器翻译而言，采用监督学习，而原句子和目标句子对输入模型进行训练，那就需要确保，Decoder在生成单词的过程中，不能看到后面的单词，这就是sequence_mask，其主要在Decoder中的self-attention中起作用。</li>
</ul>
<p>具体代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">subsequent_mask</span>(<span class="params">size</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    	Mask out subequent position</span></span><br><span class="line"><span class="string">    	这个是tgt_mask,他不让decoder过程中看到后面的单词，训练模型的过程中掩盖翻译后面的单词。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="built_in">print</span>(subsequent_mask(<span class="number">3</span>))</span><br><span class="line">    tensor([[[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">             [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">             [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]]], dtype=torch.uint8)</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    attn_shape = (1, size, size)</span></span><br><span class="line"><span class="string">    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype(&#x27;uint8&#x27;)</span></span><br><span class="line"><span class="string">    return torch.from_numpy(subsequent_mask) == 0</span></span><br></pre></td></tr></table></figure>
<h3 id="Attention-1"><a href="#Attention-1" class="headerlink" title="Attention"></a>Attention</h3><p>Transfomer的最大创新就是两种Attention，Self-Attention，Multi-Head Attention</p>
<p>Self-Attention</p>
<p>作用：简单来说，就是计算句子里每个单词受其他单词的影响程度，其最大意义在于可以学到语义依赖关系。</p>
<p>计算公式：</p>
<p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1588909761573.png" alt="1588909761573"></p>
<p>不好理解，上图</p>
<p><img src="F:\video\softmax.png" alt="softmax"></p>
<p>再看看原论文的图</p>
<p><img src="F:\video\scaled dot attention.png" alt="scaled dot attention"></p>
<p>再来看看代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention</span>(<span class="params">query, key, value, mask=<span class="literal">None</span>, dropout=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Compute &#x27;Scaled Dot Product Attention&quot;&quot;&quot;</span></span><br><span class="line">    d_k = query.size(-<span class="number">1</span>)</span><br><span class="line">    scores = torch.matmul(query, key.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / math.sqrt(d_k)</span><br><span class="line">    <span class="keyword">if</span> mask:</span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, -<span class="number">1e9</span>)</span><br><span class="line">    p_attn = F.softmax(scores, dim=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> dropout:</span><br><span class="line">        p_attn = dropout(p_attn)</span><br><span class="line">    <span class="keyword">return</span> torch.matmul(p_attn, value)</span><br></pre></td></tr></table></figure>
<p>基本参照看公式应该可以很好的看懂这些代码，具体的步骤各位就参考原论文吧。</p>
<p><strong>Multi-Head Attention</strong></p>
<p>多个Self-Attention并行计算就叫多头计算（Multi-Head Attention）模式，也就是我们俗称的多头怪，你可以将其理解为集成学习，这也是Transformer训练速度超快的原因。</p>
<p>废话不说，上图：</p>
<p><img src="F:\video\Multi-head.png" alt="Multi-head"></p>
<p>首先，我们接进来单词转化为词向量X(n,512)和W^Q, W^k, W^v(512,64)相乘，得出Q,K,V(n,64)，就生产一个Z(n,64),然后利用8个头并行操作，就生产8个Z(n,64).</p>
<p><img src="F:\video\multi-head1.png" alt="multi-head1"></p>
<p>然后将8个Z拼接起来，Z就变成了(n,512)内积W^0（512，512），输出最终的Z，传递到下一个FNN层。</p>
<p><img src="F:\video\Multi-Head 结构层.png" alt="Multi-Head 结构层"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadedAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, h, d_model, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="string">&quot;Take in model size and number of heads.&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(MultiHeadedAttention, self).__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span></span><br><span class="line">        <span class="comment"># We assume d_v always equals d_k</span></span><br><span class="line">        self.d_k = d_model // h</span><br><span class="line">        self.h = h</span><br><span class="line">        self.linears = clones(nn.Linear(d_model, d_model), <span class="number">4</span>)</span><br><span class="line">        self.attn = <span class="literal">None</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, query, key, value, mask=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;Implements Figure 2&quot;</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Same mask applied to all h heads.</span></span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        nbatches = query.size(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 1) Do all the linear projections in batch from d_model =&gt; h x d_k </span></span><br><span class="line">        query, key, value = \</span><br><span class="line">            [l(x).view(nbatches, -<span class="number">1</span>, self.h, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">             <span class="keyword">for</span> l, x <span class="keyword">in</span> <span class="built_in">zip</span>(self.linears, (query, key, value))]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2) Apply attention on all the projected vectors in batch. </span></span><br><span class="line">        x, self.attn = attention(query, key, value, mask=mask, </span><br><span class="line">                                 dropout=self.dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3) &quot;Concat&quot; using a view and apply a final linear. </span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous() \</span><br><span class="line">             .view(nbatches, -<span class="number">1</span>, self.h * self.d_k)</span><br><span class="line">        <span class="keyword">return</span> self.linears[-<span class="number">1</span>](x)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h2><p>就是数据经过Encoder-Decoder结构后还需要一个线性连接层和softmax层，这一部分预测层，命名为Generator。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    	定义输出结构，一个线性连接层+softmax层</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, vocab</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Generator, self).__init__()</span><br><span class="line">        self.proj = nn.Linear(d_model, vocab)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(self.proj(x), dim=-<span class="number">1</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>好了，这基本上把整个Transfomer进行了一个非常细致的拆分。Transformer可以说现在所有最新模型的基础，对于这一部分还是需要好好理解。</p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ul>
<li>[1] <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1706.03762.pdf">原论文</a></li>
<li>[2] <a target="_blank" rel="noopener" href="https://jalammar.github.io/illustrated-transformer/">NLP国外大牛(Jay Alammar)详解Transformer</a></li>
<li>[3] <a target="_blank" rel="noopener" href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#attention">哈佛NLP</a></li>
<li>[4] <a target="_blank" rel="noopener" href="https://juejin.im/post/5b9f1af0e51d450e425eb32d#heading-10">Blog</a></li>
<li>[5] <a target="_blank" rel="noopener" href="https://blog.csdn.net/baidu_20163013/article/details/97389827">Csdn</a></li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">ShiHai-black</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://shihai-black.github.io/2021/04/11/Transomer%D1%A7%CF%B0/">https://shihai-black.github.io/2021/04/11/Transomer%D1%A7%CF%B0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://shihai-black.github.io" target="_blank">ShiHai'Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/transfomer/">transfomer</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/04/11/%E5%A4%9A%E8%BF%9B%E7%A8%8B%E3%80%81%E5%A4%9A%E7%BA%BF%E7%A8%8B%EF%BC%88python%EF%BC%89/"><img class="prev-cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">多进程、多线程</div></div></a></div><div class="next-post pull-right"><a href="/2021/04/08/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"><img class="next-cover" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">异常检测整理</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="/img/%E5%A4%B4%E5%83%8F.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-info__name">ShiHai-black</div><div class="author-info__description">爱打球爱烧饭的猫爸爸</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">13</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">9</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/shihai-black"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Transfomer%E6%8B%86%E5%88%86"><span class="toc-number">1.</span> <span class="toc-text">Transfomer拆分</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%83%8C%E6%99%AF%E7%9F%A5%E8%AF%86"><span class="toc-number">2.</span> <span class="toc-text">背景知识</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#seq2seq"><span class="toc-number">2.1.</span> <span class="toc-text">seq2seq</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Attention"><span class="toc-number">2.2.</span> <span class="toc-text">Attention</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%AD%A3%E6%96%87"><span class="toc-number">3.</span> <span class="toc-text">正文</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Encoder-Decoder"><span class="toc-number">3.1.</span> <span class="toc-text">Encoder-Decoder</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Embedding"><span class="toc-number">3.2.</span> <span class="toc-text">Embedding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Positional-Emcoding"><span class="toc-number">3.3.</span> <span class="toc-text">Positional Emcoding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Encoder"><span class="toc-number">3.4.</span> <span class="toc-text">Encoder</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Clone"><span class="toc-number">3.4.1.</span> <span class="toc-text">Clone</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Encoder-layer"><span class="toc-number">3.4.2.</span> <span class="toc-text">Encoder_layer</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Decoder"><span class="toc-number">3.5.</span> <span class="toc-text">Decoder</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Decoder-layer"><span class="toc-number">3.5.1.</span> <span class="toc-text">Decoder layer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Mask"><span class="toc-number">3.5.2.</span> <span class="toc-text">Mask</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Attention-1"><span class="toc-number">3.5.3.</span> <span class="toc-text">Attention</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Generator"><span class="toc-number">3.6.</span> <span class="toc-text">Generator</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5"><span class="toc-number">4.</span> <span class="toc-text">参考链接</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2021/09/18/%E5%85%B3%E8%81%94%E6%8C%96%E6%8E%98/" title="关联挖掘"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="关联挖掘"/></a><div class="content"><a class="title" href="/2021/09/18/%E5%85%B3%E8%81%94%E6%8C%96%E6%8E%98/" title="关联挖掘">关联挖掘</a><time datetime="2021-09-18T08:46:37.000Z" title="发表于 2021-09-18 16:46:37">2021-09-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/09/16/Louvain%E7%AE%97%E6%B3%95/" title="Louvain算法"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Louvain算法"/></a><div class="content"><a class="title" href="/2021/09/16/Louvain%E7%AE%97%E6%B3%95/" title="Louvain算法">Louvain算法</a><time datetime="2021-09-16T13:49:43.000Z" title="发表于 2021-09-16 21:49:43">2021-09-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/08/19/GNN%E7%B3%BB%E5%88%97%E4%B9%8B-GCN/" title="GNN系列之_GCN"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="GNN系列之_GCN"/></a><div class="content"><a class="title" href="/2021/08/19/GNN%E7%B3%BB%E5%88%97%E4%B9%8B-GCN/" title="GNN系列之_GCN">GNN系列之_GCN</a><time datetime="2021-08-19T03:05:12.000Z" title="发表于 2021-08-19 11:05:12">2021-08-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/08/19/%E6%95%B0%E6%8D%AE%E6%A0%87%E5%87%86%E5%8C%96%E5%92%8C%E4%B8%AD%E5%BF%83%E5%8C%96/" title="数据标准化和中心化"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="数据标准化和中心化"/></a><div class="content"><a class="title" href="/2021/08/19/%E6%95%B0%E6%8D%AE%E6%A0%87%E5%87%86%E5%8C%96%E5%92%8C%E4%B8%AD%E5%BF%83%E5%8C%96/" title="数据标准化和中心化">数据标准化和中心化</a><time datetime="2021-08-19T02:51:19.000Z" title="发表于 2021-08-19 10:51:19">2021-08-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2021/08/17/GNN%E7%B3%BB%E5%88%97%E4%B9%8B-GraphSAGE/" title="GNN系列之_GraphSAGE"><img src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/img/default.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="GNN系列之_GraphSAGE"/></a><div class="content"><a class="title" href="/2021/08/17/GNN%E7%B3%BB%E5%88%97%E4%B9%8B-GraphSAGE/" title="GNN系列之_GraphSAGE">GNN系列之_GraphSAGE</a><time datetime="2021-08-17T13:34:49.000Z" title="发表于 2021-08-17 21:34:49">2021-08-17</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By ShiHai-black</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: '5d2G4qaJCSGGzlvVsvXc2sjI-gzGzoHsz',
      appKey: 'wpw1ViJnypU3RxqomPOuitvc',
      placeholder: 'Please leave your footprints',
      avatar: 'monsterid',
      meta: 'nick,mail,link'.split(','),
      pageSize: '10',
      lang: 'zh-CN',
      recordIP: false,
      serverURLs: '',
      emojiCDN: '',
      emojiMaps: "",
      enableQQ: false,
      path: window.location.pathname,
      requiredFields: ["nick,mail"],
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>