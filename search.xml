<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>初识NLP</title>
    <url>/2021/04/11/%E5%88%9D%E8%AF%86NLP/</url>
    <content><![CDATA[<h1 id="0、概述"><a href="#0、概述" class="headerlink" title="0、概述"></a>0、概述</h1><blockquote>
<p>此文档写于2019年</p>
</blockquote>
<p>此文档为了更好的理解自然语言处理（natural language processing），自然语言处理(NLP)就是开发能够理解人类语言的应用程序或服务。接下来会讨论一些NLP在国内外的发展状况和实际应用的一些例子，了解当前最前沿的NLP应用并掌握一定的NLP技术是接下来自己需要做的。</p>
<h1 id="1、NLP简述"><a href="#1、NLP简述" class="headerlink" title="1、NLP简述"></a>1、NLP简述</h1><p>NLP是研究人与人交际中以及在人与计算机交际中的语言问题的一门学科。NLP主要有五个主要任务：分类、匹配、翻译、结构化预测、与序贯决策过程。NLP中的绝大多数问题皆可归入其中的一个，如下表所示。在这些任务中，单词、词组、语句、段落甚至文档通常被看作标记（字符串）序列而采取相似的处理，尽管它们的复杂度并不相同。事实上，语句是 NLP 中最常用的处理单元。</p>
<table>
<thead>
<tr>
<th align="left">任务</th>
<th align="left">描述</th>
<th align="left">应用</th>
</tr>
</thead>
<tbody><tr>
<td align="left">分类</td>
<td align="left">给每个string指定一个标签</td>
<td align="left">文本分类、情感分析</td>
</tr>
<tr>
<td align="left">匹配</td>
<td align="left">匹配两个strings</td>
<td align="left">信息检索、问答系统</td>
</tr>
<tr>
<td align="left">翻译</td>
<td align="left">翻译某种语言</td>
<td align="left">机器翻译、自动语音识别</td>
</tr>
<tr>
<td align="left">结构化预测</td>
<td align="left">将每个string映射为一种结构</td>
<td align="left">命名实体识别、中文分词、词性标注、句法分析</td>
</tr>
<tr>
<td align="left">有序决策过程</td>
<td align="left">对一个动态的过程做出反应</td>
<td align="left">多伦对话系统</td>
</tr>
</tbody></table>
<h2 id="1-1应用简介"><a href="#1-1应用简介" class="headerlink" title="1.1应用简介"></a>1.1应用简介</h2><p><strong>文本分类</strong>：用电脑对文本集(或其他实体或物件)按照一定的分类体系或标准进行自动分类标记。</p>
<p><strong>情感分析</strong>：又称意见挖掘、倾向性分析等。简单而言,是对带有情感色彩的主观性文本进行分析、处理、归纳和推理的过程。</p>
<p><strong>信息检索</strong>：信息按一定的方式进行加工、整理、组织并存储起来，再根据信息用户特定的需要将相关信息准确的查找出来的过程。</p>
<p><strong>问答系统</strong>：用准确、简洁的自然语言回答用户用自然语言提出的问题。</p>
<p><strong>机器翻译</strong>：是利用计算机将一种自然语言(源语言)转换为另一种自然语言(目标语言)的过程，除了传统的文本翻译之外，还有比较特殊的手语翻译和唇语翻译。</p>
<p><strong>自动语音识别</strong>：一种将人的语音转换为文本的技术。</p>
<p><strong>命名实体识别</strong>：是指识别文本中具有特定意义的实体，主要包括人名、地名、机构名、专有名词等。</p>
<p><strong>中文分词</strong>：指的是将一个汉字序列切分成一个个单独的词。</p>
<p><strong>词性标注</strong>：利用语料库内单词的词性按其含义和上下文内容进行标记的文本数据处理技术。</p>
<p><strong>句法分析</strong>：依存句法分析接口可自动分析文本中的依存句法结构信息，利用句子中词与词之间的依存关系来表示词语的句法结构信息（如“主谓”、“动宾”、“定中”等结构关系），并用树状结构来表示整句的结构（如“主谓宾”、“定状补”等）。</p>
<p><strong>多伦对话系统</strong>：（封闭域）多轮对话是一种，在人机对话中，初步明确用户意图之后，获取必要信息以最终得到明确用户指令的方式。</p>
<h1 id="2、国内外NLP的应用"><a href="#2、国内外NLP的应用" class="headerlink" title="2、国内外NLP的应用"></a>2、国内外NLP的应用</h1><p>由于NLP领域在工业界内还属于探索状态，下表只是罗列了一下了解到的行业内较为优异的公司从事的领域和相关产品</p>
<table>
<thead>
<tr>
<th align="center"><strong>公司</strong></th>
<th align="center"><strong>关注点</strong></th>
<th align="center"><strong>产品</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="center">谷歌</td>
<td align="center">机器翻译、知识图谱（智能化搜索引擎）</td>
<td align="center">谷歌翻译、谷歌浏览器</td>
</tr>
<tr>
<td align="center">微软</td>
<td align="center">机器翻译、对话平台、中国文化、阅读理解</td>
<td align="center">微软翻译、小娜、小冰、微软对联</td>
</tr>
<tr>
<td align="center">IBM</td>
<td align="center">对话平台运用在医疗、金融、物联网</td>
<td align="center">沃森</td>
</tr>
<tr>
<td align="center">百度</td>
<td align="center">机器翻译、知识图谱、对话平台</td>
<td align="center">度秘、百度检索、小度机器人</td>
</tr>
<tr>
<td align="center">阿里</td>
<td align="center">智能导购（技术快速的跟业务对接）</td>
<td align="center">优酷、YunOS、蚂蚁金服、资讯搜索</td>
</tr>
<tr>
<td align="center">腾讯</td>
<td align="center">社交、内容、游戏</td>
<td align="center">腾讯觅影、微信、qq</td>
</tr>
</tbody></table>
<h1 id="3、NLP技术支持"><a href="#3、NLP技术支持" class="headerlink" title="3、NLP技术支持"></a>3、NLP技术支持</h1><p>接下来研究的重点在于文本分类和情感分析方向，百度，bosonnlp，阿里（收费），腾讯（收费）等公司都提供了优秀的api接口。下面是关于两方面的简单应用。</p>
<h2 id="3-1-百度API"><a href="#3-1-百度API" class="headerlink" title="3.1 百度API"></a>3.1 百度API</h2><h3 id="3-1-1-文本分类"><a href="#3-1-1-文本分类" class="headerlink" title="3.1.1 文本分类"></a>3.1.1 文本分类</h3><p>对文章按照内容类型进行自动分类，首批支持娱乐、体育、科技等26个主流内容类型，为文章聚类、文本内容分析等应用提供基础技术支持。 目前支持的一级粗粒度分类类目如下：1、国际 2、体育 3、娱乐 4、社会 5、财经 6、时事 7、科技 8、情感 9、汽车 10、教育 11、时尚 12、游戏 13、军事 14、旅游 15、美食 16、文化 17、健康养生 18、搞笑 19、家居 20、动漫 21、宠物 22、母婴育儿 23、星座运势 24、历史 25、音乐 26、综合。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> aip <span class="keyword">import</span> AipNlp</span><br><span class="line">APP_ID = <span class="string">&#x27;15438079&#x27;</span></span><br><span class="line">API_KEY = <span class="string">&#x27;rwAgYHnVN0bGkuksdvk7piio&#x27;</span></span><br><span class="line">SECRET_KEY = <span class="string">&#x27;PAw4ffLotYoUMKcOS57oF2WrOtCjWTRC&#x27;</span></span><br><span class="line">client = AipNlp(APP_ID,API_KEY, SECRET_KEY)</span><br><span class="line">title = <span class="string">&#x27;欧洲冠军杯足球赛&#x27;</span></span><br><span class="line">content = <span class="string">&quot;欧洲冠军联赛是欧洲足球协会联盟主办的年度足球比赛，代表欧洲俱乐部足球最高荣誉和水平，被认为是全世界最高素质、最具影响力以及最高水平的俱乐部赛事，亦是世界上奖金最高的足球赛事和体育赛事之一。&quot;</span></span><br><span class="line">client.topic(title, content)</span><br><span class="line"></span><br><span class="line"><span class="comment">##结果展示</span></span><br><span class="line">&#123;<span class="string">&#x27;log_id&#x27;</span>: <span class="number">6074973177044352305</span>,</span><br><span class="line"> <span class="string">&#x27;item&#x27;</span>: &#123;<span class="string">&#x27;lv2_tag_list&#x27;</span>: [&#123;<span class="string">&#x27;score&#x27;</span>: <span class="number">0.915631</span>, <span class="string">&#x27;tag&#x27;</span>: <span class="string">&#x27;足球&#x27;</span>&#125;,</span><br><span class="line">   &#123;<span class="string">&#x27;score&#x27;</span>: <span class="number">0.803507</span>, <span class="string">&#x27;tag&#x27;</span>: <span class="string">&#x27;国际足球&#x27;</span>&#125;,</span><br><span class="line">   &#123;<span class="string">&#x27;score&#x27;</span>: <span class="number">0.77813</span>, <span class="string">&#x27;tag&#x27;</span>: <span class="string">&#x27;英超&#x27;</span>&#125;],</span><br><span class="line">  <span class="string">&#x27;lv1_tag_list&#x27;</span>: [&#123;<span class="string">&#x27;score&#x27;</span>: <span class="number">0.830915</span>, <span class="string">&#x27;tag&#x27;</span>: <span class="string">&#x27;体育&#x27;</span>&#125;]&#125;&#125;</span><br></pre></td></tr></table></figure>

<h3 id="3-1-2-情感倾向分析"><a href="#3-1-2-情感倾向分析" class="headerlink" title="3.1.2 情感倾向分析"></a>3.1.2 情感倾向分析</h3><p>对包含主观观点信息的文本进行情感极性类别（积极、消极、中性）的判断，并给出相应的置信度。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> aip <span class="keyword">import</span> AipNlp</span><br><span class="line">APP_ID = <span class="string">&#x27;15438079&#x27;</span></span><br><span class="line">API_KEY = <span class="string">&#x27;rwAgYHnVN0bGkuksdvk7piio&#x27;</span></span><br><span class="line">SECRET_KEY = <span class="string">&#x27;PAw4ffLotYoUMKcOS57oF2WrOtCjWTRC&#x27;</span></span><br><span class="line">client = AipNlp(APP_ID,API_KEY, SECRET_KEY)</span><br><span class="line">text = <span class="string">&quot;苹果是一家伟大的公司&quot;</span></span><br><span class="line">client.sentimentClassify(text)</span><br><span class="line"></span><br><span class="line"><span class="comment">##结果展示</span></span><br><span class="line">&#123;<span class="string">&#x27;log_id&#x27;</span>: <span class="number">9106725229255700145</span>,</span><br><span class="line"> <span class="string">&#x27;text&#x27;</span>: <span class="string">&#x27;苹果是一家伟大的公司&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;items&#x27;</span>: [&#123;<span class="string">&#x27;positive_prob&#x27;</span>: <span class="number">0.727802</span>,   //表示属于积极类别的概率</span><br><span class="line">   <span class="string">&#x27;confidence&#x27;</span>: <span class="number">0.395115</span>,   //表示分类的置信度</span><br><span class="line">   <span class="string">&#x27;negative_prob&#x27;</span>: <span class="number">0.272198</span>,   //表示属于消极类别的概率</span><br><span class="line">   <span class="string">&#x27;sentiment&#x27;</span>: <span class="number">2</span>&#125;]&#125;    //表示情感极性分类结果</span><br></pre></td></tr></table></figure>

<h3 id="3-1-3-百度API优势"><a href="#3-1-3-百度API优势" class="headerlink" title="3.1.3 百度API优势"></a>3.1.3 百度API优势</h3><ol>
<li><strong>永久免费</strong>。2018年5月11日，百度自然语言处理技术宣布对外永久免费，且不限调用量。</li>
<li><strong>功能丰富</strong>。包括词法分析、依存句法分析、词向量表示、DNN语言模型、词义相似度、短文本相似度、评论观点抽取、情感倾向分析、文章标签、文章分类等。</li>
<li><strong>接口易用</strong>。标准化接口封装，通过云计算调用可快速使用工具，降低开发人力成本。</li>
</ol>
<h2 id="3-2-bosonnlp"><a href="#3-2-bosonnlp" class="headerlink" title="3.2 bosonnlp"></a>3.2 bosonnlp</h2><h3 id="3-2-1-新闻分类"><a href="#3-2-1-新闻分类" class="headerlink" title="3.2.1 新闻分类"></a>3.2.1 新闻分类</h3><p>将新闻文本归类到预设的 14 个分类当中：0、体育 1、教育 2、财经 3、社会 4、娱乐 5、军事 6、国内 7、科技 8、互联网 9、房地产 10、国际 11、女人 12、汽车 13、游戏。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from bosonnlp import BosonNLP</span><br><span class="line">import os</span><br><span class="line">nlp &#x3D; BosonNLP(&#39;HpikDzL0.32936.Hi3mJFXh1P1b&#39;)</span><br><span class="line">nlp.classify([&#39;俄否决安理会谴责叙军战机空袭阿勒颇平民&#39;,</span><br><span class="line">            &#39;邓紫棋谈男友林宥嘉：我觉得我比他唱得好&#39;,</span><br><span class="line">             &#39;Facebook收购印度初创公司&#39;])</span><br><span class="line">             </span><br><span class="line">##结果展示</span><br><span class="line">[5, 4, 8]</span><br></pre></td></tr></table></figure>

<h3 id="3-2-2-情感分析"><a href="#3-2-2-情感分析" class="headerlink" title="3.2.2 情感分析"></a>3.2.2 情感分析</h3><p>其接口将文本的情感分为负面和非负面两类。本引擎用微博、新闻、汽车、餐饮等不同行业语料进行标注和机器学习，调用时请通过 URL 参数选择特定的模型，以获得最佳的情感判断准确率。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> bosonnlp <span class="keyword">import</span> BosonNLP</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">nlp = BosonNLP(<span class="string">&#x27;HpikDzL0.32936.Hi3mJFXh1P1b&#x27;</span>)</span><br><span class="line">s = [<span class="string">&#x27;苹果是一家垃圾公司&#x27;</span>, <span class="string">&#x27;美好的世界&#x27;</span>]</span><br><span class="line">result = nlp.sentiment(s)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"></span><br><span class="line"><span class="comment">##结果展示</span></span><br><span class="line">[[<span class="number">0.01819305539338867</span>, <span class="number">0.9818069446066113</span>], [<span class="number">0.92706110187413</span>, <span class="number">0.07293889812586994</span>]]</span><br></pre></td></tr></table></figure>

<h3 id="3-2-3-bosonnlp的优势"><a href="#3-2-3-bosonnlp的优势" class="headerlink" title="3.2.3 bosonnlp的优势"></a>3.2.3 bosonnlp的优势</h3><p>BOSON实验室的优点在于对于某一个单文本或者多文本，他能够得出一个<strong>综合的分析结果</strong>。</p>
<ol>
<li><strong>单文本分析</strong>：只输入一段文本，输出的分析结果包括词性分析、实体识别、依存文法、情感分析、新闻摘要、新闻分类、关键词提取、语义联系。</li>
<li><strong>多文本分析</strong>：同时输入多段文本、新闻，输出的分析结果包括话题聚类、情感分析、词云图、词频图。</li>
</ol>
<h1 id="4、总结"><a href="#4、总结" class="headerlink" title="4、总结"></a>4、总结</h1><h2 id="4-1-NLP最新进展"><a href="#4-1-NLP最新进展" class="headerlink" title="4.1 NLP最新进展"></a>4.1 NLP最新进展</h2><h3 id="4-1-1-文本分类通用规律"><a href="#4-1-1-文本分类通用规律" class="headerlink" title="4.1.1 文本分类通用规律"></a>4.1.1 文本分类通用规律</h3><p>2018年7月，谷歌做了45万次不同类型的文本分类后，总结出一个对于文本分类和情感分析而言通用的“模型选择算法”。</p>
<p><strong>文本分类的流程图</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">graph LR</span><br><span class="line">搜集数据--&gt;探索数据</span><br><span class="line">探索数据--&gt;选择模型</span><br><span class="line">选择模型--&gt;准备数据</span><br><span class="line">准备数据--&gt;构建训练和评估模型</span><br><span class="line">构建训练和评估模型--&gt;调优超参数</span><br><span class="line">调优超参数--&gt;部署模型</span><br></pre></td></tr></table></figure>

<p>我们预处理数据的方式将取决于我们选择的模型。模型可以大致分为两类：使用单词排序信息的模型（序列模型），以及仅将文本视为单词的“bags”（sets）的模型（n-gram模型）。</p>
<ul>
<li><p><strong>序列模型</strong>:卷积神经网络（CNN），递归神经网络（RNN）及其变体。</p>
</li>
<li><p><strong>n-gram模型</strong>:逻辑回归（LR）,支持向量机（SVM）,梯度提升树等。</p>
</li>
</ul>
<p><strong>结论</strong>：</p>
<p>在实验中，其观察到“样本数”（S）与“每个样本的单词数”（W）的比率与模型的性能具有相关性。</p>
<p>当该<strong>比率的值很小（&lt;1500）</strong>时，以n-gram作为输入的小型多层感知机表现得更好，或者说至少与序列模型一样好。 MLP易于定义和理解，而且比序列模型花费的计算时间更少。（感觉如果数据量特别大的python这个工具也不一定能用）</p>
<p>当此<strong>比率的值很大（&gt; = 1500）</strong>时，使用序列模型。</p>
<h3 id="4-1-2-BERT"><a href="#4-1-2-BERT" class="headerlink" title="4.1.2 BERT"></a>4.1.2 BERT</h3><p>2018年10月，谷歌新发布的一种模型<strong>BERT</strong>(Bidirectional Encoder Representations from Transformers)，在NLP业内引起巨大反响。BERT在机器阅读理解测试SQuAD1.1中表现出惊人的成绩：全部两个衡量指标上全面超越人类，并且还在11种不同NLP任务中创出最佳成绩。</p>
<p><strong>具体做法：</strong></p>
<ol>
<li>采取新的预训练的目标函数：the “masked language model” (MLM) 对于mask随机输入一些单词（80%的时间真的用[MASK]取代被选中的词，10%的时间用一个随机词取代它，10%的时间保持不变），然后在预训练中对它们进行预测。这样做的好处是学习到的表征能够融合两个方向上的内容。</li>
<li>增加句子级别的任务：“next sentence prediction”。作者同时预训练了一个“next sentence prediction”任务。具体做法是随机替换一些句子，然后利用上一句进行此句是否为下一句的预测。</li>
</ol>
<p><strong>模型框架</strong>：</p>
<p>BASE：L=12, H=768, A=12, Total Parameters=110M</p>
<p>LARGE：L=24, H=1024, A=16, Total Parameters=340M</p>
<p>这里的L为神经网络层数，H为隐藏向量参数，A为自注意力头数</p>
<p><strong>结论</strong>：</p>
<ul>
<li>在大数据中效果好，但如果模型经过足够的预训练，在小任务中大模型也能增长。</li>
<li>计算成本相当高，作者动用了谷歌 Cloud AI 资源，用了 64 颗 TPU，算了 4 天，模型参数寻优的训练过程才收敛。</li>
</ul>
<h2 id="4-2-自我学习要求"><a href="#4-2-自我学习要求" class="headerlink" title="4.2 自我学习要求"></a>4.2 自我学习要求</h2><ul>
<li>熟悉NLP处理基本技术，熟练运用API解决相关问题。</li>
<li>熟悉文本分类的过程，掌握NLP领域常用的机器学习模型。</li>
<li>学习深度学习算法和神经网络相关模型，学习TensorFlow框架。</li>
</ul>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title>异常检测整理</title>
    <url>/2021/04/08/%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/</url>
    <content><![CDATA[<p><strong>定义</strong>：识别不正常情况与挖掘非逻辑数据的技术，也叫outliers。</p>
<p><strong>前提</strong>：</p>
<ol>
<li>异常数据只占少数</li>
<li>异常数据特征值和正常数据差别很大</li>
</ol>
<p><strong>应用领域</strong>：</p>
<ol>
<li>CV领域:抖音发现违规视频</li>
<li>数据挖掘：信用卡盗刷，支付宝，异常金额支出。</li>
</ol>
<p><strong>模型</strong></p>
<ol>
<li>无监督学习、AutoEncoder、GAN、矩阵因子分解</li>
<li>半监督学习，强化学习</li>
<li>hybrid（混种）、特征提取+传统算法</li>
<li>单分类神经网路(MLM)</li>
</ol>
<h2 id="统计学方法"><a href="#统计学方法" class="headerlink" title="统计学方法"></a>统计学方法</h2><h3 id="1-3sigma-箱形图"><a href="#1-3sigma-箱形图" class="headerlink" title="1. 3sigma/箱形图"></a>1. 3sigma/箱形图</h3><p><strong>原理</strong>：远离3sigma（拉依达准则）数据概率低于0.01，认为这些数据为异常值</p>
<p><strong>缺点</strong>：</p>
<ol>
<li>要保证异常值较少</li>
<li>只能检测单维数据</li>
<li>要假定数据服从正态分布或近似</li>
</ol>
<h3 id="2-高斯概率密度异常检测算法（1999）"><a href="#2-高斯概率密度异常检测算法（1999）" class="headerlink" title="2. 高斯概率密度异常检测算法（1999）"></a>2. 高斯概率密度异常检测算法（1999）</h3><p><strong>原理</strong>：首先，该算法假设数据集服从高斯分布的，然后再分别计算训练集在空间中的重心, 和方差, 然后根据高斯概率密度估算每个点被分配到重心的概率，进而完成异常检测任务。(感觉和3sigma想法很像)</p>
<p><strong>缺点</strong>：</p>
<ol>
<li>不适用于高维特征数据集</li>
<li>要求数据大致服从高斯分布的数据集</li>
</ol>
<h2 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h2><h3 id="1-Isolation-Forest-孤立森林"><a href="#1-Isolation-Forest-孤立森林" class="headerlink" title="1. Isolation Forest(孤立森林)"></a>1. Isolation Forest(孤立森林)</h3><p><strong>定义</strong>：孤立森林是用于异常检测的机器学习算法。这是一种无监督学习算法，通过隔离数据中的离群值识别异常</p>
<p><strong>原理</strong>：孤立森林通过<strong>随机选择特征</strong>，然后<strong>随机选择</strong>特征的<strong>分割值</strong>，递归地生成数据集的分区。和数据集中「正常」的点相比，要隔离的异常值所需的随机分区更少，因此<strong>异常值是树中路径更短的点</strong>，路径长度是从根节点经过的边数。</p>
<p><strong>重要参数</strong></p>
<ul>
<li>n_estimators：树的数量</li>
<li>max_sample:样本抽样（小样本全抽）</li>
<li>contamination：异常占比，这个值很关键</li>
<li>max_features：随机选取特征维度</li>
</ul>
<p><strong>具体步骤</strong>：</p>
<ol>
<li>从数据集中按max_sample进行抽样</li>
<li>随机指定部分维度（论文是只用一个维度），在当前节点数据中随机产生一个切割点p——切割点产生于当前节点数据中指定维度的最大值和最小值之间。</li>
<li>以此切割点生成了一个超平面，然后将当前节点数据空间划分为2个子空间：把指定维度里小于p的数据放在当前节点的左边，把大于等于p的数据放在当前节点的右边。</li>
<li>在子节点中递归步骤2和3，不断构造新的子节点，直到子节点中只有一个数据（无法再继续切割）或子节点已到达限定高度（算法设定的）。</li>
</ol>
<p><strong>优点</strong>：</p>
<ol>
<li>节省内存。由于其主要定位异常值，因此其不要求树全部描述出来，可以用最大深度来限制。并且其不像kmeans等需要计算有关距离、密度的指标，可大幅度提升速度。</li>
<li>适用于小数据集：因此采样，如果数据集很大，</li>
<li>集成算法，多个专家的树针对不同的异常</li>
</ol>
<p><strong>缺点：</strong></p>
<ol>
<li>如果只能用1个维度，那对于图片这种高维特征，效果就不佳。</li>
</ol>
<p><strong>参考</strong>：</p>
<p><a href="https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf">Isolation Forest</a>[2008]</p>
<p><a href="https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/tkdd11.pdf">Isolation-based Anomaly Detection</a>[2012] </p>
<h3 id="2-Local-Outlier-Factor（局部异常因子算法）"><a href="#2-Local-Outlier-Factor（局部异常因子算法）" class="headerlink" title="2. Local Outlier Factor（局部异常因子算法）"></a>2. Local Outlier Factor（局部异常因子算法）</h3><p><strong>定义</strong>：一种典型的基于密度的高精度离群点检测算法</p>
<p><strong>原理</strong>： LOF算法是通过比较每个点p和邻域点的密度来判断该点是否为异常：点p的密度越低，越有可能是异常点。而点的密度是通过点之间的距离来计算的，点之间距离越远，密度越低；距离越近，密度越高。也就是说，LOF算法中点的密度是通过点的k邻域计算得到的，而不是通过全局计算得到，这里的”k邻域”也就是该算法中“局部”的概念。</p>
<p><strong>重要参数</strong></p>
<ul>
<li>n_neighbors：上文的K，检测的领域点个数超过样本数则使用所有的样本进行检测</li>
<li>contamination：异常占比，这个值很关键</li>
<li>metric:距离度量单位</li>
<li>p=2：距离度量单位（l1,l2分别为1,2）</li>
</ul>
<p><strong>具体步骤</strong>：主要就是计算LOF，如果想弄明白怎么算的直接看源论文</p>
<p><strong>优点</strong></p>
<ol>
<li>适用于对不同密度的数据的异常检测</li>
</ol>
<p><strong>缺点</strong></p>
<ol>
<li>检测的数据必须有明显的密度差异，计算较为复杂</li>
</ol>
<p><strong>参考</strong></p>
<p><a href="https://www.dbs.ifi.lmu.de/Publikationen/Papers/LOF.pdf">Local Outlier Factor</a>[2000]</p>
<h3 id="3-One-Class-SVM（新颖点检测算法）"><a href="#3-One-Class-SVM（新颖点检测算法）" class="headerlink" title="3. One Class SVM（新颖点检测算法）"></a>3. One Class SVM（新颖点检测算法）</h3><p><strong>定义</strong>：无监督算法，将数据分类为不同的类型</p>
<p><strong>原理</strong>：与SVM类似，SVM是寻找一个超平面，使用这个超平面把正常数据和异常数据划分开。而One_class SVM是基于一类数据（正常数据）求超平面，对SVM算法中求解负样本最大间隔目标进行改造。</p>
<p><strong>优点</strong>：</p>
<ol>
<li>适用于高纬数据集，毕竟是超平面</li>
</ol>
<h3 id="4-DBSCAN（密度聚类）"><a href="#4-DBSCAN（密度聚类）" class="headerlink" title="4. DBSCAN（密度聚类）"></a>4. DBSCAN（密度聚类）</h3><p><strong>定义</strong>：无监督聚类算法，按照密度将空间内的数据进行聚类，如果单独成簇就是异常值。</p>
<p><strong>原理</strong>：给定一个距离半径和类内最少多少个点，然后把可以满足的点全部都连起来，判定为同类。</p>
<p><strong>优点</strong>：</p>
<ol>
<li>不需要知道K，按距离自动划分为簇</li>
<li>能发现任意非球形状的簇类</li>
<li>对输入样本的顺序并不敏感</li>
</ol>
<p><strong>缺点</strong>：</p>
<ol>
<li>不适用于高纬数据</li>
<li>时间复杂度较高</li>
</ol>
<p><strong>参考</strong></p>
<p><a href="http://www2.cs.uh.edu/~ceick/7363/Papers/dbscan.pdf">DBSCAN</a></p>
<h2 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h2><h3 id="1-PCA"><a href="#1-PCA" class="headerlink" title="1. PCA"></a>1. PCA</h3><p><strong>定义</strong>：主成分分析（PCA），一种使用广泛的数据降维算法，主要思想是将n维特征映射到k维上，k维是全新的正交特征也称为主成分。</p>
<p><strong>原理</strong>：PCA的工作就是从原始的空间中顺序地寻找一组相互正交的坐标轴，第一个新坐标轴选择是原始数据中<strong>方差最大</strong>的方向，第二个新坐标轴选取是与第一个坐标轴正交的平面中使得方差最大的，第三个轴是与第1,2个轴正交的平面中方差最大的。依次类推，可以得到n个这样的坐标轴。取前k个包含绝大部分方差的坐标轴。</p>
<p><strong>实现方式</strong></p>
<ul>
<li>基于特征值分解协方差矩阵实现</li>
<li>基于SVD分解协方差矩阵（大样本高效，因此sklearn里面也是用svd分解）</li>
</ul>
<p><strong>具体步骤</strong></p>
<ol>
<li>中心化（norm_x）</li>
<li>获取协方差矩阵（np.dot(norm_xT,norm_x）)</li>
<li>计算协方差矩阵的特征值和特征向量(feature_n,vectors_n)</li>
<li>按特征值排序，选取最大的K组特征（特征值越大方差越大）(feature_k,vectors_k)</li>
<li>反转，np.dot(norm_x,vectors_k)</li>
</ol>
<p><strong>特点</strong></p>
<ol>
<li>PCA后的特征不具有解释性</li>
<li>PCA不适合线性不可分数据</li>
</ol>
<h2 id="半监督学习（AE类）"><a href="#半监督学习（AE类）" class="headerlink" title="半监督学习（AE类）"></a>半监督学习（AE类）</h2><h3 id="1-Auto-Encoder（鼻祖）"><a href="#1-Auto-Encoder（鼻祖）" class="headerlink" title="1. Auto-Encoder（鼻祖）"></a>1. Auto-Encoder（鼻祖）</h3><p><strong>背景</strong>：AE的思想最早是1986年被提出来的</p>
<p><strong>定义</strong>：一种无监督的学习算法，其可以解决PCA无法解决的问题，因为其可以对线性不可分的数据进行降维，利用反向传播算法，让目标等于输入值。</p>
<p><strong>原理</strong>：构建一个函数使X和Y的Loss最小，保证降维后的图片保持最大的信息。</p>
<p><strong>重建误差</strong>：就是损失函数，原始X和重建的Y之间的差异称为重建误差</p>
<p><strong>异常检测的使用</strong></p>
<ol>
<li>基于偏差的<strong>半监督学习的</strong>异常检测方法，使用<strong>重建误差</strong>作为异常分数，具有高重建误差作为异常。</li>
<li>因此在训练阶段仅使用正常数据，训练之后AE可以很好的重建正常数据，而AE未遇到的异常数据则会重建失败（定位异常图片）</li>
<li>为了增强鲁棒性一般会在正常数据里撒入一些噪声、或者随机将某些值变0（有点像dropout）。</li>
</ol>
<p><strong>为什么叫Encoder，明明有Decoder</strong>：</p>
<p>​    Vincent在2010的论文中做了研究，发现只要encoder单组W就可以，decoder中的W1可以用encoder中的W转置获得。其证明，W1没有任何作用，完全没有必要训练。</p>
<h3 id="2-Denoising-Auto-Encoder（降噪自编码）"><a href="#2-Denoising-Auto-Encoder（降噪自编码）" class="headerlink" title="2. Denoising Auto-Encoder（降噪自编码）"></a>2. Denoising Auto-Encoder（降噪自编码）</h3><p><strong>背景</strong>：Vincent在2008年的《Extracting and Composing Robust Features with Denoising Autoencoders》提出该模型</p>
<p><strong>做法</strong>：对输入数据加入噪声，而输出数据是正常的数据，DAE会要求模型只去学习主要特征，输出的数据会有更好的鲁棒性</p>
<h3 id="3-Sparse-Auto-Encoder（稀疏自编码）"><a href="#3-Sparse-Auto-Encoder（稀疏自编码）" class="headerlink" title="3. Sparse Auto-Encoder（稀疏自编码）"></a>3. Sparse Auto-Encoder（稀疏自编码）</h3><p><strong>背景</strong>：Andrew在2011年的《Sparse autoencoder》提出该模型</p>
<p><strong>做法</strong>：在普通的AE的基础上增加了稀疏性约束，即要求神经元的平均输出较低，如果激活函数是sigmod，尽量让隐藏神经元输出为0，如果激活函数是tanh，尽量把输出变为-1.</p>
<blockquote>
<p>KL散度（相对熵）： </p>
<ol>
<li>一种衡量两个概率分布的匹配程度的指标，两个分布差异越大，KL散度就越大。p目标分布，q是匹配分布，如果两个分布完全匹配，KL散度为0</li>
<li>KL散度是非对称的，即D（p||q）不一定等于D（q||p）</li>
<li>KL散度又叫相对熵，在信息论中，对于D（p||q）,描述的是q去拟合p产生的信息损耗</li>
</ol>
</blockquote>
<h3 id="4-Variational-Auto-Encoder（VAE）"><a href="#4-Variational-Auto-Encoder（VAE）" class="headerlink" title="4. Variational Auto-Encoder（VAE）"></a>4. Variational Auto-Encoder（VAE）</h3><p><strong>背景</strong>：2014年，用于异常检测是2015年</p>
<p><strong>重建概率</strong>：通过导出原始输入变量分布的参数的随机潜变量来计算重建概率</p>
<p><strong>损失函数</strong>：重建概率+KL散度</p>
<p><strong>与AE的区别和联系</strong>：</p>
<ol>
<li>AE中的潜在变量由确定性映射定义，然而，由于VAE使用概率编码器来模拟潜在变量的分布，因此可以在采样过程中考虑潜在空间的可变性</li>
<li>重建是随机变量，重建概率不仅考虑重建与原始输入之间的差异，而且还考虑分布函数的参数来重建的可变性。</li>
<li>重建概率的计算不需要对异构数据进行处理，其重建误差的阈值比AE更客观且易于理解。</li>
</ol>
<p><strong>异常检测的使用</strong></p>
<ol>
<li>一个半监督框架，仅使用正常实例的数据来训练VAE</li>
<li>抽取样本，对于来自encoder的每个样本，概率解码器输出均值和方差参数，使用这些参数，计算从分布产生原始数据的概率。</li>
</ol>
<h3 id="5-Conditional-VAE"><a href="#5-Conditional-VAE" class="headerlink" title="5. Conditional VAE"></a>5. Conditional VAE</h3><p><strong>背景</strong>：一种框架，加入某种图片的先验信息，提升训练效果。</p>
<p><strong>优化点</strong>：</p>
<ul>
<li><strong>多尺度预测目标</strong>（内容很多，并行/串行，输入输出端……）：Loss=Loss1（1/4图）+Loss2（1/2图）+Loss3（原图）</li>
<li><strong>对KL散度进行近似</strong>：给KL散度增加罚函数，简单定为：batch_size/样本数量</li>
<li><strong>增加label的one-hot</strong>：对encode和decode的输入加入label的one-hot（conditional就是这一条，但对于一场检测没有意义）</li>
</ul>
<p><strong>与VAE的联系与区别</strong>：</p>
<ol>
<li>基本还是VAE的结构，对encoder和decoder的输入增加先验信息。</li>
<li>Loss函数进行了相应的调整。</li>
</ol>
<h3 id="A-Deep-Hierarchical-Variational-Auto-Encoder"><a href="#A-Deep-Hierarchical-Variational-Auto-Encoder" class="headerlink" title="A Deep Hierarchical Variational Auto-Encoder"></a>A Deep Hierarchical Variational Auto-Encoder</h3><h2 id="半监督学习（GAN类）"><a href="#半监督学习（GAN类）" class="headerlink" title="半监督学习（GAN类）"></a>半监督学习（GAN类）</h2><h3 id="1-AnoGAN"><a href="#1-AnoGAN" class="headerlink" title="1. AnoGAN"></a>1. AnoGAN</h3><p><strong>背景</strong>：GAN用于异常检测的开山之作，2017的论文</p>
<p><strong>基本思想</strong></p>
<ol>
<li>训练阶段：塞入正常图片，利用DCGAN训练一个模型，希望生成器能够生成足够好的正常图片，好到辨别器也无法判别他到底对不对。</li>
<li>测试阶段：固定生成器和辨别器，他希望在Z的潜藏空间中找到一个和X最像的映射，然后利用梯度下降法，更新Z，生成一张由潜藏空间生成且和X最像的图片。<ul>
<li>定义一个损失函数，代表潜藏空间生成的图片和X的差异。</li>
<li>随机抽一个Z，利用梯度下降法，不断更新使损失函数变最小，然后利用最好的Z生成图片进行异常分数计算（或者直接用）</li>
</ul>
</li>
</ol>
<p><strong>缺点：</strong></p>
<ol>
<li>对于X到Z的映射没有再训练阶段完成</li>
<li>z的更新非常耗时，并且每张图片都需要更新。</li>
<li>该异常检测应该只适用于有严格边界的图像中。</li>
</ol>
<p><strong>结果</strong>：</p>
<p>原论文效果并不适用（周末试试用经典数据集进行训练）。可能因为生成模型训练的并不到位。但是loss也已经不变化了</p>
<h3 id="2-WGAN"><a href="#2-WGAN" class="headerlink" title="2. WGAN"></a>2. WGAN</h3><p><strong>背景</strong>：令GAN研究者眼前一亮的一种新模型，其从理论上很好的解决了GAN的几大问题.</p>
<p><strong>GAN的表面问题</strong></p>
<ol>
<li>训练不稳定，不容易收敛<ul>
<li>判别器训练太好，生成器梯度消失，loss不下降。</li>
<li>判别器训练不好，生成器不稳定。</li>
</ul>
</li>
<li>生成图片的多样性不足。</li>
</ol>
<p><strong>GAN的本质问题</strong></p>
<ol>
<li>等价优化的距离衡量（KL散度、JS散度）不合理（改进办法：用Wasserstein距离代替JS散度，合理解决前两个问题）</li>
<li>生成分布于真实分布没有重叠部分（改进办法：增加噪声）</li>
</ol>
<p><strong>WGAN和GAN的区别</strong></p>
<ol>
<li>判别器没有sigmoid，他不想DCGAN是2分类，他是回归问题</li>
<li>生成器和判别器的loss不取log</li>
<li>每次更新判别器的参数之后把它们的绝对值截断到不超过一个固定常数c</li>
<li>不用Adam，用rmsprop做优化器</li>
</ol>
<p><strong>WGAN_GP和WGAN的区别</strong></p>
<p><em>WGAN的问题</em></p>
<p>由于其限制每次判别器更新参数绝对值不超过阈值c,按原论文0.01，因此会出现大量参数集中在0.01和-0.01，这就导致所有参数基本都是0.01和-0.01。判别器性能就会变得很差。</p>
<p><em>WGAN_GP的优势</em></p>
<p>其设置一个额外的Loss来限制判别器的梯度，将参数更新正态分布到（-0.01，0.01）内。</p>
<p><em>改变</em></p>
<ol>
<li>优化器有可以用Adam了。。。</li>
<li>由于其对每个样本独立的施加梯度惩罚，所以判别器的模型架构中不能使用Batch Normalization，改为使用Layer Normalization</li>
</ol>
<h3 id="3-f-AnoGAN"><a href="#3-f-AnoGAN" class="headerlink" title="3. f-AnoGAN"></a>3. f-AnoGAN</h3><p><strong>背景</strong>：AnoGan的优化版本，2019年的论文。AnoGAN存在一个问题，每一张图片都需要不断迭代训练一个Z来代表其在隐空间内的点。接着利用训练好的DCGAN进行异常检测。由于其迭代优化势必导致时间加剧。而f-AnoGAN通过引入Encoder，解决了这个问题。</p>
<p><strong>基本思想</strong>：</p>
<ol>
<li><p>输入正常图片，利用WGAN_GP进行训练生成器和判别器。</p>
</li>
<li><p>输入正常图片，对Encoder+训练好的WGAN进行优化。提出了三种训练Encoder的方式（3种损失函数）</p>
<ul>
<li>izi：image-z-image1，AutoEncoder样式的损失函数</li>
<li>ziz:z-image-z1，将Encoder放到Generate后面，比较z与z1的损失函数</li>
<li>izif：就是Anogan中的损失函数，从像素和特征两个维度比较图片结果。</li>
</ul>
<p>作者实验证明izif效果最好，并且整体训练也很快。</p>
</li>
</ol>
<p><strong>结果</strong>：</p>
<p>WGAN没有收敛，两边都不断降低，不确定是不是GP哪里有问题。</p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>异常检测</tag>
      </tags>
  </entry>
  <entry>
    <title>多进程、多线程</title>
    <url>/2021/04/11/%E5%A4%9A%E8%BF%9B%E7%A8%8B%E3%80%81%E5%A4%9A%E7%BA%BF%E7%A8%8B%EF%BC%88python%EF%BC%89/</url>
    <content><![CDATA[<h1 id="多进程、多线程（python）"><a href="#多进程、多线程（python）" class="headerlink" title="多进程、多线程（python）"></a>多进程、多线程（python）</h1><p>[TOC]</p>
<blockquote>
<p>此文章写于2020年</p>
</blockquote>
<p>​    这周接到1个需求，项目经理觉得我这边构造数据太过缓慢，由于数据量过大，以前数据构造完后将其导入MPP，利用copy_from速度还是很快的，一般为10W/s。现在换成Kafka消息队列，又由于python库自带的原因（这个是组里大神告诉我的），无法像java开发利用list导数据，因此我只能一条一条以json的形式将数据放入消息队列，一般为800条/s。原先构造5000W数据需要7H，经过优化后暂时需要3.5H，性能提升一倍。有点小开心，话不多说，石海同学开始将一下个人对于标题的理解。。。。</p>
<p>​    进一步优化利用多进程的JoinableQueue，一边一直产生数据，另一边一直消费数据，现在5000W数据大约需要1.5H，性能较最初提升了近五倍。</p>
<h2 id="历史"><a href="#历史" class="headerlink" title="历史"></a>历史</h2><p>上古年代：在很久很久以前，当时主流的磁盘操作系统MS-DOS是只支持单任务操作的，就打个比方，如果我想在电脑上听音乐和看电影，是不能同时开启的，只能先听音乐后看电影，或者位置互换。</p>
<p>2002：横空出世的Intel Pentium 4HT处理器，提出了cpu多线程（SMT），其支持一个cpu进行多任务开启。</p>
<p>————你总不能要求Guido1989年为了打发圣诞节假期发明的一种编译语言还要设计一下多线程的部分。</p>
<p>2006：在秋季的英特尔信息技术峰会上，Inter总裁宣布，其在11月份将会交付第一台4核cpu处理器，而这距离双核发布还不到1年，支持多核时代就此拉开序幕。</p>
<p>————为什么说python多线程是历史遗留问题，因为在当时想要在两个或者更多的处理器对同一个对象运行时，为了保护线程数据完整性和状态同步，最简单的方法就是加锁，于是出现了GIL这把超级大锁。</p>
<p>2006-至今：GPU也浩浩荡荡发展了十几年了，过程我就不细说了（因为不懂），总之对于图象类任务GPU和CPU不是一个量级上的，以前看过一个视频，很好的解释了CPU和GPU的区别。比如我要画一幅图，cpu需要一笔一划将他画出来，而GPU是直接在脑子里构思好，一炮就把图打出来了。因此对于神经网络这个模型，多个神经元同时进行计算，GPU比CPU快太多。。。。。</p>
<h2 id="编译器、解释器、IDE"><a href="#编译器、解释器、IDE" class="headerlink" title="编译器、解释器、IDE"></a>编译器、解释器、IDE</h2><p><strong>编译器</strong>：对于C、C++这类语言需要先编译后运行，其中编译的时候就是编译器。</p>
<p><strong>解释器</strong>：对于python、PHP、JavaScript等就是利用解释器，’一边编译一边解释‘，不过速度会比较慢，因此产生一种叫预编译的东西，python在运行时就先生成pyc的二进制临时文件，在出结果。</p>
<p>预编译：Java、C#，运行前将源代码编译成中间代码，然后再解释器运行，这种执行效率比编译运行会有效率一些，避免重复编译。</p>
<p><strong>IDE</strong>：集成<a href="https://baike.baidu.com/item/%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83">开发环境</a>（<a href="https://baike.baidu.com/item/IDE">IDE</a>，Integrated Development Environment ），用于提供程序开发环境的应用程序，python常用的就是pycharm和jupyter notebook。</p>
<h2 id="GIL锁"><a href="#GIL锁" class="headerlink" title="GIL锁"></a>GIL锁</h2><p><strong>概念：</strong>GIL全称Global Interpreter Lock,其并不是python的特性，由于Cpython是大部分环境下默认的python执行环境，而在实现python解释器会自动上锁。</p>
<p><strong>目的：</strong>确保每个进程只有一个线程运行，所以在外面我们一般说python的多线程是伪线程。因为不管你有几个核，你用多线程也只能跑一个核。</p>
<h2 id="进程、线程、协程的利用"><a href="#进程、线程、协程的利用" class="headerlink" title="进程、线程、协程的利用"></a>进程、线程、协程的利用</h2><p><strong>进程</strong>：拥有代码和打开的文件资源、数据资源、独立的内存空间。</p>
<p><strong>线程</strong>：从属于进程，是程序的实际执行者，线程拥有自己的栈空间。</p>
<p><strong>协程</strong>：从属于线程，是一种比线程更加轻量级的存在。</p>
<p><strong>总结：</strong></p>
<p><strong>对操作系统来说，线程是最小的执行单元，进程是最小的资源管理单元。</strong></p>
<h2 id="多进程"><a href="#多进程" class="headerlink" title="多进程"></a>多进程</h2><p>我觉得最简单的讲，多进程就是你在任务窗口看建了几个任务。比如你电脑上有16个核，理论上你可以开16个进程，每个进程占满一个cpu。对python而言由于没有多线程的利用，所有在单进程无法满足需求时，自然得利用多进程。</p>
<h3 id="单任务单进程"><a href="#单任务单进程" class="headerlink" title="单任务单进程"></a>单任务单进程</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Process</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">work</span>(<span class="params">name</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;name&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    p = Process(target=work,args=(<span class="string">&#x27;single task&#x27;</span>,))</span><br><span class="line">    p.start()</span><br><span class="line">    p.join()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>我还看到一种是用run不用start的，但大概看了下没什么特殊的，不过这个join只能用于start.所以建议还是都用start，这个join是阻塞的意思，简单来说：主进程和其他子进程结束的话，都等着，等我结束了才能结束。</p>
<p>举个例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Process</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">work1</span>(<span class="params">name</span>):</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;name&#125;</span>:<span class="subst">&#123;os.getpid()&#125;</span>&#x27;</span>)</span><br><span class="line">        time.sleep(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">work2</span>(<span class="params">name</span>):</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;<span class="subst">&#123;name&#125;</span>:<span class="subst">&#123;os.getpid()&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    p1 = Process(target=work1,args=(<span class="string">&#x27;single task1&#x27;</span>,))</span><br><span class="line">    p2 = Process(target=work2,args=(<span class="string">&#x27;single task2&#x27;</span>,))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;主进程:<span class="subst">&#123;os.getpid()&#125;</span>&#x27;</span>)</span><br><span class="line">    p1.start()</span><br><span class="line">    p1.join()</span><br><span class="line">    p2.start()</span><br><span class="line">    p2.join()</span><br></pre></td></tr></table></figure>

<p>如果这么写，主进程你走你的，work2子进程你给我等一下。等我run完你在run。</p>
<p><strong>注意：</strong>每个子进程之间的join不是串行的，是并行的。既无论你多少个子进程，谁最后结束，谁关闭文件。</p>
<h3 id="单任务多进程"><a href="#单任务多进程" class="headerlink" title="单任务多进程"></a>单任务多进程</h3><p>比如你要启动50个进程，然后写50个子进程就太慢了。因此，对于多进程有一种新的形式。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">long_time_task</span>(<span class="params">name</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Run task %s (%s)...&#x27;</span> % (name, os.getpid()))</span><br><span class="line">    start = time.time()</span><br><span class="line">    time.sleep(random.random() * <span class="number">3</span>)</span><br><span class="line">    end = time.time()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Task %s runs %0.2f seconds.&#x27;</span> % (name, (end - start)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Parent process %s.&#x27;</span> % os.getpid())</span><br><span class="line">    p = Pool(<span class="number">4</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        p.apply_async(long_time_task, args=(i,))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Waiting for all subprocesses done...&#x27;</span>)</span><br><span class="line">    p.close()</span><br><span class="line">    p.join()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;All subprocesses done.&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">########################################################</span></span><br><span class="line"><span class="comment">#运行结果</span></span><br><span class="line"><span class="comment">########################################################</span></span><br><span class="line">Parent process <span class="number">7748.</span></span><br><span class="line">Waiting <span class="keyword">for</span> <span class="built_in">all</span> subprocesses done...</span><br><span class="line">Run task <span class="number">0</span> (<span class="number">7780</span>)...</span><br><span class="line">Run task <span class="number">1</span> (<span class="number">7781</span>)...</span><br><span class="line">Run task <span class="number">2</span> (<span class="number">7782</span>)...</span><br><span class="line">Run task <span class="number">3</span> (<span class="number">7783</span>)...</span><br><span class="line">Run task <span class="number">4</span> (<span class="number">7781</span>)...</span><br><span class="line">All subprocesses done.</span><br></pre></td></tr></table></figure>

<p>可以看到task1和task4的进程是一样的，因为只启动了4个进程，因此第5个进程需要等待其他进程结束才开始运行。</p>
<p><strong>注意：</strong>close不加直接写join是会报错的。</p>
<h3 id="多任务多进程"><a href="#多任务多进程" class="headerlink" title="多任务多进程"></a>多任务多进程</h3><p>我这边用的是经典的生产者和消费者模型，及一个模型构造生产者，一个模型构造消费者，两者通过自带的JoinableQueue进行通信。（不要用deque里面的队列！！）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span>  multiprocessing <span class="keyword">import</span> JoinableQueue,Process</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">producer</span>(<span class="params">q,name,food</span>):</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        time.sleep(random.random())</span><br><span class="line">        fd = <span class="string">&#x27;%s%s&#x27;</span>%(food,i+<span class="number">1</span>)</span><br><span class="line">        q.put(fd)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;%s生产了一个%s&#x27;</span>%(name,food))</span><br><span class="line">    q.join()  <span class="comment"># 我启动了生产者之后，生产者函数一直在生成数据，直到生产完所有数据将队列q.join()一下，意思是当我生产的数据都被消费者消费完之后 队列的阻塞才结束。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">consumer</span>(<span class="params">q,name</span>):</span>  <span class="comment"># 消费者不需要像Queue那样判断从队列里拿到None再退出执行消费者函数了</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        food = q.get()</span><br><span class="line">        time.sleep(random.random())</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;%s吃了%s&#x27;</span>%(name,food))</span><br><span class="line">        q.task_done()  <span class="comment"># 消费者每次从队列里面q.get()一个数据，处理过后就使用队列.task_done()</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    jq = JoinableQueue()</span><br><span class="line">    p =Process(target=producer,args=(jq,<span class="string">&#x27;喜洋洋&#x27;</span>,<span class="string">&#x27;包子&#x27;</span>))</span><br><span class="line">    p.start()</span><br><span class="line">    c = Process(target=consumer,args=(jq,<span class="string">&#x27;灰太狼&#x27;</span>))</span><br><span class="line">    c.daemon = <span class="literal">True</span>  <span class="comment"># 守护进程，如果用Pool就不用这个也没事</span></span><br><span class="line">    c.start()</span><br><span class="line">    p.join()  <span class="comment"># 阻塞生产者</span></span><br></pre></td></tr></table></figure>

<p>具体实现情况</p>
<ol>
<li>启动一个生产者，和一个消费者（这里可以用多进程消费），看具体时间</li>
<li>生产者结束后将JoinableQueue进行阻塞，直到队列全部被消费后，才结束进程。</li>
</ol>
<p>Queue与JoinableQueue的区别</p>
<ol>
<li>Queue在消费者中需要if判断队列，不然的话就陷入死循环。而jionableQueue不用，其只需要将队列堵塞后，队列消费完程序才解决。</li>
<li>Queue很难控制，因为如果只是判断队列是否存在，就需要考虑产生和消耗队列的时间。因此我认为还是JoinableQueue好</li>
</ol>
<h2 id="多线程"><a href="#多线程" class="headerlink" title="多线程"></a>多线程</h2><p>开头就说了，python虽然线程是真正的线程，但在解释器执行的时候会遇到GIL锁，导致其无论多少核，只能跑满一个核，所以对于比较简单的可以运行多线程，好像在爬虫里面运用多线程的比较多。</p>
<p>那为什么有的程序一个进程一开用了N个cpu，那是因为他进程中存在利用c扩写的库，他将关键的部分用C/C++写成python扩展，其他还用python写。一般计算密集型的程序都会用C代码编写并通过扩展的方式集成到python脚本（numpy）。在扩展中完全可以用C创建原生线程，而且不用锁GIL，充分利用CPU的计算资源。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> threading  <span class="keyword">import</span> Thread</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">work1</span>():</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;work1 has working&#x27;</span>)</span><br><span class="line">    time.sleep(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">work2</span>():</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;work2 has working&#x27;</span>)</span><br><span class="line">    time.sleep(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">work3</span>():</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;work3 has working&#x27;</span>)</span><br><span class="line">    time.sleep(<span class="number">8</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    start_time =time.time()</span><br><span class="line">    p1 = Thread(target=work1)</span><br><span class="line">    p2 = Thread(target=work2)</span><br><span class="line">    p3 = Thread(target=work3)</span><br><span class="line">    p1.start()</span><br><span class="line">    p2.start()</span><br><span class="line">    p3.start()</span><br><span class="line">    p3.join()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;cost time <span class="subst">&#123;time.time()-start_time&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">能否利用Pool批量构造生产者和消费者，</span><br><span class="line">暂时只能做到Pool构造消费者，多个Process构造生产者进行生产。。。</span><br><span class="line">（除非使用process分别构造生产者和消费者模型）</span><br></pre></td></tr></table></figure>









]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title>Transomer学习</title>
    <url>/2021/04/11/Transomer%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<p>[TOC]</p>
<h1 id="Transfomer拆分"><a href="#Transfomer拆分" class="headerlink" title="Transfomer拆分"></a>Transfomer拆分</h1><p>为了更好的学习当前NLP主流模型，如Bert，GPT2及Bert一系列的衍生物，Transfomer是这一系列的基础。因此本文的主要目的是记录个人基于一些博客和原论文对Transfomer模型进行拆分的结果。</p>
<p><strong>目的</strong>：减少计算量并提高并行效率，同时不减弱最终的实验结果。</p>
<p><strong>创新点</strong>：</p>
<ol>
<li>Self-attention</li>
<li>Multi-Head Attention</li>
</ol>
<h1 id="1-背景知识"><a href="#1-背景知识" class="headerlink" title="1. 背景知识"></a>1. 背景知识</h1><h2 id="1-1-seq2seq"><a href="#1-1-seq2seq" class="headerlink" title="1.1 seq2seq"></a>1.1 seq2seq</h2><p>定义：seq2seq模型是采用一系列项目(单词、字母、图像特征等)并输出另一个项目序列的模型。在机器翻译中，序列是一系列单词，经过seq2seq后，输出同样是一系列单词。</p>
<p><video src="F:\video\seq2seq_2.mp4"></video></p>
<p>接下来我们掀开这个model内部，该模型主要由一个Encoder和一个Decoder组成。</p>
<ul>
<li>Encoder：处理输入序列的每个项目，捕捉载体中的信息（context）。</li>
<li>Decoder：处理完整序列后，Encoder将信息（context）传递至Decoder，并且开始逐项生产输出序列。</li>
</ul>
<p><video src="F:\video\seq2seq_4.mp4"></video></p>
<p>而context是一个向量，其大小基于等同于编码器中RNN的隐藏神经元。</p>
<p>在单词输入之前，我们需要将单词转化为词向量，其可以通过word2vec等模型进行预训练训练词库，然后将单词按照词库的词向量简单提取即可，在上面的资历中，其处理过程如下：</p>
<p><img src="F:\video\embedding.png" alt="embedding"></p>
<p>这里简单将其设为维度4，通常设为200或300，在此，简单展示一下RNN的实现原理。</p>
<p><video src="F:\video\RNN_1.mp4"></video></p>
<p>利用先前的输入的隐藏状态，RNN将其输出至一个新的隐藏状态，接下来我们看看seq2seq中的隐藏状态是怎么进行的。</p>
<p><video src="F:\video\seq2seq_5.mp4"></video></p>
<p>Encoder中最后一个hidden-state实际就是前文提到的context，接下来我们进一步拆解，展示其具体细节。</p>
<p><video src="F:\video\seq2seq_6.mp4"></video></p>
<p>总结：由于在Encoder阶段，每个单词输入都会产生一个新的hidden_state，最后输出一个context给Decoder进行解码。因此，当文本内容过长时，容易丢失部分信息，为了解决这个问题，Attention应运而生。</p>
<h2 id="1-2-Attention"><a href="#1-2-Attention" class="headerlink" title="1.2 Attention"></a>1.2 Attention</h2><p>Attention这个概念最早出现在《Neural machine traslation by jointly learning to align and translate》论文中，其后《Neural image caption generation with visual attention》对attention形式进行总结。</p>
<p>定义：为了解决文本过长信息丢失的问题，相较于seq2seq模型，attention最大的区别就是他不在要求把所以信息都编入最后的隐藏状态中，而是可以在编码过程中，对每一个隐藏状态进行保留，最后在解码的过程中，每一步都会选择性的从编码的隐藏状态中选一个和当前状态最接近的子集进行处理，这样在产生每一个输出时就能够充分利用输入序列携程的信息，下面很好的展示了attention在seq2seq模型中运用。</p>
<p><video src="F:\video\seq2seq_7.mp4"></video></p>
<p>接下来，我们放大一下decoder对于attention后隐藏状态的具体使用，其在每一步解码均要进行。</p>
<ol>
<li>查看在attention机制中每个具体的隐藏状态，选出其与句子中的那个单词最相关。</li>
<li>给每个隐藏状态打分。</li>
<li>将每个隐藏状态进行softmax以放大具有高分数的隐藏状态。</li>
<li>进行总和形成attention输入Decoder的向量。</li>
</ol>
<p><video src="F:\video\attention_process.mp4"></video></p>
<p>最后，将整个过程放一起，以便更好的理解attention机制(代码实现的时候进一步理解)。</p>
<ol>
<li>Decoder输入：初始化一个解码器隐藏状态+经过预训练后的词向量。</li>
<li>将前两者输入到RNN中，产生一个新的隐藏状态h4和输出，将输出丢弃(seq2seq是直接将context送入下一个RNN作为输入)。</li>
<li>attention：利用h4和encoder层简历的隐藏状态进行计算context（c4）。</li>
<li>将c4和h4进行concatenate。</li>
<li>将其结果通过前向神经网络，输出一个结果。</li>
<li>该结果表示当前时间输出的对应文字。</li>
<li>重复下一个步骤。</li>
</ol>
<p><video src="F:\video\attention_tensor_dance.mp4"></video></p>
<p>注意：该模型并不是将输入和输出的单词一一对应，他有可能一个单词对应两个单词甚至影响第三个单词，这是经过训练得到的。</p>
<h1 id="2-正文"><a href="#2-正文" class="headerlink" title="2.正文"></a>2.正文</h1><p>总算要写到Transformer部分了，有点小激动，让我们一起来看看这个影响到现在的模型到底长啥样，为了便于理解，我这边会结合代码+论文进行讲解。</p>
<p>首先，引入原论文的结构图</p>
<p><img src="F:\video\结构图.png" alt="结构图"></p>
<p>看不懂不要紧，这边引入代码的结构图</p>
<p><img src="F:\video\代码结构图.png" alt="代码结构图"></p>
<h2 id="2-1-Encoder-Decoder"><a href="#2-1-Encoder-Decoder" class="headerlink" title="2.1 Encoder-Decoder"></a>2.1 Encoder-Decoder</h2><p>从宏观角度来看，Transformer与Seq2Seq的结构相同，依然引入经典的Encoder-Decoder结构，只是其中的神经层已经不是以前的RNN和CNN，而是完全引入注意力机制来进行构建。</p>
<p><img src="F:\video\stack.png" alt="stack"></p>
<p>上图代码的复现结构</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderDecoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        整体来说Transformer还是Encoder和Decoder结构，其中包括两个Embedding，一块Encoder，一块Decoder，一个输出层</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, encoder, decoder, src_embed, tgt_embed, generator</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(EncoderDecoder, self).__init__()</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line">        self.src_embed = src_embed</span><br><span class="line">        self.tgt_embed = tgt_embed</span><br><span class="line">        self.generator = generator</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, src, tgt, src_mask, tgt_mask</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;Take in and process masked src and target sequences.&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span>(<span class="params">self, src, src_mask</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.encoder(self.src_embed(src), src_mask)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span>(<span class="params">self, memory, src_mask, tgt, tgt_mask</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)</span><br></pre></td></tr></table></figure>

<p>接下来，我将按照结构顺序一一介绍</p>
<h2 id="2-2-Embedding"><a href="#2-2-Embedding" class="headerlink" title="2.2 Embedding"></a>2.2 Embedding</h2><p>这一层没什么好说的倒是，就是一个Embedding层。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Embeddings</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">	<span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">		将单词转化为词向量</span></span><br><span class="line"><span class="string">	&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, vocab</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Embeddings, self).__init__()</span><br><span class="line">        self.lut = nn.Embedding(vocab, d_model)</span><br><span class="line">        self.d_model = d_model</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.lut(x) * math.sqrt(self.d_model)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>困惑</strong>：为什么要乘以sqrt(d_model),希望有大神给予指点！</p>
<h2 id="2-3-Positional-Emcoding"><a href="#2-3-Positional-Emcoding" class="headerlink" title="2.3 Positional Emcoding"></a>2.3 Positional Emcoding</h2><p>由于Transfomer完全引入注意力机制，其不像CNN和RNN会对输入单词顺序自动打上标签，其无法输出每个单词的顺序，在机器翻译中可是爆炸的啊，举个例子，你输入一句：我欠你的一千万不用还了，他返回一句：你欠我的一千万不用还了，那不是血崩。</p>
<p>为了解决这个问题，Transfomer在Encoder过程中为每个输入单词打上一个位置编码，然后在Decoder将位置变量信息添加，该方法不用模型训练，直接按规则进行添加。</p>
<p>我们看看原论文里的图</p>
<p><img src="F:\video\Emcodeing.png" alt="Emcodeing"></p>
<p>其计算的具体公式在原论文3.5，</p>
<p><img src="F:\video\Emcoding公式.png" alt="Emcoding公式"></p>
<p>对于上述公式，代码中进行了对数转化，具体如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    	位置变量公式详见https://arxiv.org/abs/1706.03762(3.5)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, dropout, max_len=<span class="number">5000</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        <span class="comment"># pe 初始化为0，shape为 [n, d_model] 的矩阵，用来存放最终的PositionalEncoding的</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        <span class="comment"># position 表示位置，shape为 [max_len, 1]，从0开始到 max_len</span></span><br><span class="line">        position = torch.arange(<span class="number">0.</span>, max_len).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 这个是变形得到的，shape 为 [1, d_model//2]</span></span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0.</span>, d_model, <span class="number">2</span>) * -(math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        <span class="comment"># 矩阵相乘 (max_len, 1) 与 (1, d_model // 2) 相乘，最后结果 shape   (max_len, d_model // 2)</span></span><br><span class="line">        <span class="comment"># 即所有行，一半的列。（行为句子的长度，列为向量的维度）</span></span><br><span class="line">        boy = position * div_term</span><br><span class="line">        <span class="comment"># 偶数列  奇数列 分别赋值</span></span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(boy)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(boy)</span><br><span class="line">        <span class="comment"># 为何还要在前面加一维？？？</span></span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># Parameter 会在反向传播时更新</span></span><br><span class="line">        <span class="comment"># Buffer不会，是固定的，本文就是不想让其被修改</span></span><br><span class="line">        self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># x能和后面的相加说明shape是一致的，也是 (1, sentence_len, d_model)</span></span><br><span class="line">        <span class="comment"># forward 其实就是，将原来的Embedding 再加上 Positional Embedding</span></span><br><span class="line">        x += Variable(self.pe[:, :x.size(<span class="number">1</span>)], requires_grad=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure>

<h2 id="2-4-Encoder"><a href="#2-4-Encoder" class="headerlink" title="2.4 Encoder"></a>2.4 Encoder</h2><p>Encoder代码结构</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;核心Encoder是由N层堆叠而成&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, layer, N</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, mask</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        将输入x和掩码mask逐层传递下去，最后再 LayerNorm 一下。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure>

<p>Encoder块主要就是利用Clone函数重复构建相同结构的Encoder_layer.</p>
<h3 id="2-4-1-Clone"><a href="#2-4-1-Clone" class="headerlink" title="2.4.1 Clone"></a>2.4.1 Clone</h3><p>整个Encoder部分由N个Encoder_layer堆积二乘，为了复刻每个层的结构，构建克隆函数。(Decoder类似)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clones</span>(<span class="params">module, N</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    	copyN层形成一个list</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N)])</span><br></pre></td></tr></table></figure>

<h3 id="2-4-2-Encoder-layer"><a href="#2-4-2-Encoder-layer" class="headerlink" title="2.4.2 Encoder_layer"></a>2.4.2 Encoder_layer</h3><p>接下来，我们在将独立的Encoder_layer进行拆分，看看里面到底是什么东西。</p>
<p><img src="F:\video\encoderlayer.png" alt="encoderlayer"></p>
<p>上图很清晰的展示Encoder_layer内部的结构，X1，X2是经过转化的词向量，经过Positional Emcoding进入Encider_layer,喝口水，这部分要讲的有点多……</p>
<p>首先，Encoder_layer分为上下两层，第一层包含self-attention+SublayerConnection,第二层为FNN+SublayerConnection。Attention的内容我后面再说，这里先讲下什么是SublayerConnection。attention的内容我后面再说，这里先讲下什么是SublayerConnection。</p>
<p><strong>SublayerConnection</strong></p>
<p>SublayerConnection内部设计基于两个核心：</p>
<ol>
<li>Residual_Connection（残差连接），这是上图的Add</li>
<li>Layer Normalize(层级归一化)，这是上图的Normalize</li>
</ol>
<p>如下图所示，残差连接，就是在原图正常的结构下，将X进行保留，最终得出的结果是X+F(x).</p>
<p><strong>优势</strong>：反向传播过程中，对X求导会多出一个常数1，梯度连乘，不会出现梯度消失(详细的内容等我以后探究一下)</p>
<p>![Residual connection](F:\video\Residual connection.png)</p>
<p>具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SublayerConnection</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">		A residual connection followed by a layer norm.Note for code simplicity the norms is first as opposed to last.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, size, dropout</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SublayerConnection, self).__init__()</span><br><span class="line">        self.norm = LayerNorm(size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, sublayer</span>):</span></span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sublayer(self.norm(x)))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>Layer Normalize</strong></p>
<p>归一化最简单的理解就是将输入转化为均值为0，方差为1的数据，而由于NLP中Sequence长短不一，LN相较于BN在RNN这种类似的结构效果会好很多。其具体的概括就是：对每一个输入样本进行归一化操作，保证该样本在多维度符合均值0，方差为1.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LayerNorm</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;构建一个 layernorm层，具体细节看论文 https://arxiv.org/abs/1607.06450</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, features, eps=<span class="number">1e-6</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LayerNorm, self).__init__()</span><br><span class="line">        self.a_2 = nn.Parameter(torch.ones(features))</span><br><span class="line">        self.b_2 = nn.Parameter(torch.zeros(features))</span><br><span class="line">        self.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        mean = x.mean(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        std = x.std(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self.a_2 * (x - mean) / (std + self.eps) + self.b_2</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>FNN</strong></p>
<p>前馈神经网络，这就不细说了，具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionwiseFeedForward</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, d_ff, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        self.w_1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        self.w_2 = nn.Linear(d_ff, d_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.w_2(self.dropout(F.relu(self.w_1(x))))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>放张图，清晰一点</p>
<p><img src="F:\video\FNN.png" alt="FNN"></p>
<h2 id="2-5-Decoder"><a href="#2-5-Decoder" class="headerlink" title="2.5 Decoder"></a>2.5 Decoder</h2><p>与Encoder结构类似，其由N个Decoder_layer组成(Transformer中N=6),相较于Encoder,其接受的参数多了Encoder生成的memory以及目标句子中的掩码gt_mask.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">	<span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">		Gener is N layer decoder with masking</span></span><br><span class="line"><span class="string">	&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, layer, N</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, memory, src_mask, tgt_mask</span>):</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, memory, src_mask, tgt_mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure>

<h3 id="2-5-1-Decoder-layer"><a href="#2-5-1-Decoder-layer" class="headerlink" title="2.5.1 Decoder layer"></a>2.5.1 Decoder layer</h3><p>下图很好的表明了Decoder_layer的区别</p>
<p><img src="F:\video\Decoder.png" alt="Decoder"></p>
<p>简单的解释一下，每一个Decoder_layer有三层组成</p>
<p>第一层：Self-Attention+SublayerConnection</p>
<p>第二层：Encoder-Decoder Attention+SublayerConnection(Decoder独有)</p>
<p>第三层：FFN+SublayerConnection</p>
<p>具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, size, self_attn, src_attn, feed_forward, dropout</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer, self).__init__()</span><br><span class="line">        self.size = size</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.src_attn = src_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, memory, src_mask, tgt_mask</span>):</span></span><br><span class="line">        m = memory</span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, tgt_mask))</span><br><span class="line">        x = self.sublayer[<span class="number">1</span>](x, <span class="keyword">lambda</span> x: self.src_attn(x, m, m, src_mask))</span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">2</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure>

<h3 id="2-5-2-Mask"><a href="#2-5-2-Mask" class="headerlink" title="2.5.2 Mask"></a>2.5.2 Mask</h3><p><strong>作用：</strong>Mask简单来说就是掩码的意思，在我们这里的意思大概就是对某些值进行掩盖，不使其产生效果。</p>
<p>文中的Mask主要包括<strong>两种</strong>：</p>
<ul>
<li>src_mask(padding_mask):由于Sequence长短不一，利用Padding对其填充为0，然后在对其进行attention的过程中，这些位置没有意义的，src_mask的作用就是不将Attention机制放在这些位置上进行处理，这就是padding_mask,其基于作用于所有attention。</li>
<li>tgt_mask(sequence_mask):对于机器翻译而言，采用监督学习，而原句子和目标句子对输入模型进行训练，那就需要确保，Decoder在生成单词的过程中，不能看到后面的单词，这就是sequence_mask，其主要在Decoder中的self-attention中起作用。</li>
</ul>
<p>具体代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">subsequent_mask</span>(<span class="params">size</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    	Mask out subequent position</span></span><br><span class="line"><span class="string">    	这个是tgt_mask,他不让decoder过程中看到后面的单词，训练模型的过程中掩盖翻译后面的单词。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="built_in">print</span>(subsequent_mask(<span class="number">3</span>))</span><br><span class="line">    tensor([[[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">             [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">             [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]]], dtype=torch.uint8)</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    attn_shape = (1, size, size)</span></span><br><span class="line"><span class="string">    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype(&#x27;uint8&#x27;)</span></span><br><span class="line"><span class="string">    return torch.from_numpy(subsequent_mask) == 0</span></span><br></pre></td></tr></table></figure>

<h3 id="2-5-3-Attention"><a href="#2-5-3-Attention" class="headerlink" title="2.5.3 Attention"></a>2.5.3 Attention</h3><p>Transfomer的最大创新就是两种Attention，Self-Attention，Multi-Head Attention</p>
<p>Self-Attention</p>
<p>作用：简单来说，就是计算句子里每个单词受其他单词的影响程度，其最大意义在于可以学到语义依赖关系。</p>
<p>计算公式：</p>
<p><img src="C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\1588909761573.png" alt="1588909761573"></p>
<p>不好理解，上图</p>
<p><img src="F:\video\softmax.png" alt="softmax"></p>
<p>再看看原论文的图</p>
<p>![scaled dot attention](F:\video\scaled dot attention.png)</p>
<p>再来看看代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention</span>(<span class="params">query, key, value, mask=<span class="literal">None</span>, dropout=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Compute &#x27;Scaled Dot Product Attention&quot;&quot;&quot;</span></span><br><span class="line">    d_k = query.size(-<span class="number">1</span>)</span><br><span class="line">    scores = torch.matmul(query, key.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / math.sqrt(d_k)</span><br><span class="line">    <span class="keyword">if</span> mask:</span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, -<span class="number">1e9</span>)</span><br><span class="line">    p_attn = F.softmax(scores, dim=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> dropout:</span><br><span class="line">        p_attn = dropout(p_attn)</span><br><span class="line">    <span class="keyword">return</span> torch.matmul(p_attn, value)</span><br></pre></td></tr></table></figure>

<p>基本参照看公式应该可以很好的看懂这些代码，具体的步骤各位就参考原论文吧。</p>
<p><strong>Multi-Head Attention</strong></p>
<p>多个Self-Attention并行计算就叫多头计算（Multi-Head Attention）模式，也就是我们俗称的多头怪，你可以将其理解为集成学习，这也是Transformer训练速度超快的原因。</p>
<p>废话不说，上图：</p>
<p><img src="F:\video\Multi-head.png" alt="Multi-head"></p>
<p>首先，我们接进来单词转化为词向量X(n,512)和W^Q, W^k, W^v(512,64)相乘，得出Q,K,V(n,64)，就生产一个Z(n,64),然后利用8个头并行操作，就生产8个Z(n,64).</p>
<p><img src="F:\video\multi-head1.png" alt="multi-head1"></p>
<p>然后将8个Z拼接起来，Z就变成了(n,512)内积W^0（512，512），输出最终的Z，传递到下一个FNN层。</p>
<p>![Multi-Head 结构层](F:\video\Multi-Head 结构层.png)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadedAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, h, d_model, dropout=<span class="number">0.1</span></span>):</span></span><br><span class="line">        <span class="string">&quot;Take in model size and number of heads.&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(MultiHeadedAttention, self).__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span></span><br><span class="line">        <span class="comment"># We assume d_v always equals d_k</span></span><br><span class="line">        self.d_k = d_model // h</span><br><span class="line">        self.h = h</span><br><span class="line">        self.linears = clones(nn.Linear(d_model, d_model), <span class="number">4</span>)</span><br><span class="line">        self.attn = <span class="literal">None</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, query, key, value, mask=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="string">&quot;Implements Figure 2&quot;</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># Same mask applied to all h heads.</span></span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        nbatches = query.size(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 1) Do all the linear projections in batch from d_model =&gt; h x d_k </span></span><br><span class="line">        query, key, value = \</span><br><span class="line">            [l(x).view(nbatches, -<span class="number">1</span>, self.h, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">             <span class="keyword">for</span> l, x <span class="keyword">in</span> <span class="built_in">zip</span>(self.linears, (query, key, value))]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2) Apply attention on all the projected vectors in batch. </span></span><br><span class="line">        x, self.attn = attention(query, key, value, mask=mask, </span><br><span class="line">                                 dropout=self.dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3) &quot;Concat&quot; using a view and apply a final linear. </span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous() \</span><br><span class="line">             .view(nbatches, -<span class="number">1</span>, self.h * self.d_k)</span><br><span class="line">        <span class="keyword">return</span> self.linears[-<span class="number">1</span>](x)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="2-6-Generator"><a href="#2-6-Generator" class="headerlink" title="2.6 Generator"></a>2.6 Generator</h2><p>就是数据经过Encoder-Decoder结构后还需要一个线性连接层和softmax层，这一部分预测层，命名为Generator。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    	定义输出结构，一个线性连接层+softmax层</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, vocab</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Generator, self).__init__()</span><br><span class="line">        self.proj = nn.Linear(d_model, vocab)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(self.proj(x), dim=-<span class="number">1</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>好了，这基本上把整个Transfomer进行了一个非常细致的拆分。Transformer可以说现在所有最新模型的基础，对于这一部分还是需要好好理解。</p>
<h1 id="3-参考链接"><a href="#3-参考链接" class="headerlink" title="3. 参考链接"></a>3. 参考链接</h1><ul>
<li>[1] <a href="https://arxiv.org/pdf/1706.03762.pdf">原论文</a></li>
<li>[2] <a href="https://jalammar.github.io/illustrated-transformer/">NLP国外大牛(Jay Alammar)详解Transformer</a></li>
<li>[3] <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#attention">哈佛NLP</a></li>
<li>[4] <a href="https://juejin.im/post/5b9f1af0e51d450e425eb32d#heading-10">Blog</a></li>
<li>[5] <a href="https://blog.csdn.net/baidu_20163013/article/details/97389827">Csdn</a></li>
</ul>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>transfomer</tag>
      </tags>
  </entry>
</search>
